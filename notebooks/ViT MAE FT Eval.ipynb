{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e71a03-1175-4dda-a061-bc691ae74a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from lightning import Trainer\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054b71d3-d45f-4774-9c0c-19d1f3eb52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/buehlern/Documents/Masterarbeit/models')\n",
    "from src.data.mri_datamodule import MRIDataModule\n",
    "from src.models.vit_mae_module import VisionTransformerMAE\n",
    "from src.models.vit_mae_probe_module import MAEFineProbeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed73786f-4423-440e-bb41-c37880ee8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "mae_name = \"ViT-L MAE\" # Old pretrained model\n",
    "mae_name = \"ViT-L MAE FT-1\" # FT PT Model normal, 50 epochs\n",
    "mae_name = \"ViT-L MAE FT-2\" # FT PT Model normal, 90 epochs\n",
    "mae_name = \"ViT-L MAE FT-3\" # FT PT Model normal, 300 epochs\n",
    "mae_name = \"ViT-L MAE FT-4\" # FT PT Model normal, 1000 epochs\n",
    "mae_name = \"ViT-L MAE FT-5\" # FT PT Model normal, 10k samples, 3 epochs\n",
    "mae_name = \"ViT-L_MAE_FT-10k_1\" # FT PT Model normal, 10k samples, 10 epochs\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1093ed-7315-47f3-bde8-4d9375ffc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(mae_checkpoint)\n",
    "state_dict = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a3eebc-1994-4bf2-9ac5-69c35cf2020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f43334-7b17-4923-9e0e-579ff102f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(state_dict, unwanted_prefix, new_prefix):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(unwanted_prefix):\n",
    "            new_key = key.replace(unwanted_prefix, new_prefix)\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return collections.OrderedDict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f72f022-1013-4ca2-a2fd-70a13c3b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = remap_keys(state_dict, 'net._orig_mod.', 'net.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f6955b-1e78-4f20-bad5-5407299affc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc887bf6-fe26-448f-b742-782f5d8aaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = VisionTransformerMAE(image_size = 3072, patch_size = 48, image_channels=1, output_attentions=True)\n",
    "mae.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a2b4bd-ba0e-4d96-890b-8d7e695159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerMAE(\n",
       "  (net): ViTMAEForPreTraining(\n",
       "    (vit): ViTMAEModel(\n",
       "      (embeddings): ViTMAEEmbeddings(\n",
       "        (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "          (projection): Conv2d(1, 1024, kernel_size=(48, 48), stride=(48, 48))\n",
       "        )\n",
       "      )\n",
       "      (encoder): ViTMAEEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x ViTMAELayer(\n",
       "            (attention): ViTMAEAttention(\n",
       "              (attention): ViTMAESelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTMAESelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTMAEIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTMAEOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): ViTMAEDecoder(\n",
       "      (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (decoder_layers): ModuleList(\n",
       "        (0-7): 8 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (decoder_pred): Linear(in_features=512, out_features=2304, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_loss): MeanSquaredError()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3757896c-6ad3-4c1d-9f2c-f724bba1f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable masking\n",
    "#mae.net.config.mask_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1352bd16-65db-44fd-b7bd-0b66fc4c7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_attentions: True\n",
      "_attn_implementation: eager\n"
     ]
    }
   ],
   "source": [
    "# For outputting attentions\n",
    "print('output_attentions:', mae.net.config.output_attentions)\n",
    "print('_attn_implementation:', mae.net.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733f1ff2-9a6a-40ff-a48d-29239a4de188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label bodypart as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_test_114.pkl...\n",
      "MRIDatasetBase(len=114) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 92\n",
      "Getting val indices...\n",
      "Done. Val len: 6\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 16\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the DataModule\n",
    "mri_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_test_114\",\n",
    "    label=\"bodypart\",\n",
    "    batch_binning=\"smart\",\n",
    "    batch_bins=[1152, 1536, 1920, 2304, 2688, 3072],\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.05,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617949f2-82d8-4ea2-8dd9-20c5f180f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 1]\n",
    "    assert image.shape[2] == 1\n",
    "    plt.imshow(image, cmap=plt.cm.bone)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56503f00-3c36-45f3-9d5b-ffda7674b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pixel_values, model, imgname=None):\n",
    "    patch_size = model.config.patch_size\n",
    "    print(pixel_values.size())\n",
    "    image_width, image_height = pixel_values.size()[-2:]\n",
    "    num_patches_x = image_width // patch_size\n",
    "    num_patches_y = image_height // patch_size\n",
    "    print(f\"Size: {image_width}x{image_height}\")\n",
    "    print(f\"Patches: {num_patches_x}x{num_patches_y}\")\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(pixel_values, interpolate_pos_encoding=True, output_attentions=True)\n",
    "    y = model.unpatchify(outputs.logits, original_image_size=pixel_values.size()[-2:])\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "    \n",
    "    # visualize the mask\n",
    "    mask = outputs.mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 *1)  # (N, H*W, p*p*1)\n",
    "    mask = model.unpatchify(mask, original_image_size=pixel_values.size()[-2:])\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    \n",
    "    x = torch.einsum('nchw->nhwc', pixel_values)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # Attention map calculations\n",
    "    # seq_len = 1025\n",
    "    # Without [CLS] token: seq_len = 1024\n",
    "    # num_patches_x_out = num_patches_y = sqrt(seq_len) = sqrt(1024) = 32\n",
    "    # num_patches_x_in = image_size // patch_size = 3072 // 48 = 64\n",
    "    # Reason for difference is masking! 4096 * (1-0.75) = 1024\n",
    "\n",
    "    #image_size = model.config.image_size\n",
    "    #patch_size = model.config.patch_size\n",
    "    #num_patches = image_size // patch_size\n",
    "    #attn_map = attn.squeeze().numpy()\n",
    "    ## Scale attention map to original image size\n",
    "    #attn_map_scaled = np.zeros((image_size, image_size))\n",
    "    #for i in range(num_patches):\n",
    "    #    for j in range(num_patches):\n",
    "    #        x_start = i * patch_size\n",
    "    #        x_end = x_start + patch_size\n",
    "    #        y_start = j * patch_size\n",
    "    #        y_end = y_start + patch_size\n",
    "    #        \n",
    "    #        attn_map_scaled[x_start:x_end, y_start:y_end] = attn_map[i * num_patches + j]\n",
    "    \n",
    "    #num_patches = attn.size(-1)\n",
    "    #print(f\"num_patches (after masking {model.config.mask_ratio}):\", num_patches) # 1024\n",
    "    #attn = attn[0].reshape(model.config.patch_size, model.config.patch_size, -1)\n",
    "    # Rescale attention to image size\n",
    "    #attn = F.interpolate(attn.unsqueeze(0).unsqueeze(0), scale_factor=model.config.patch_size, mode=\"nearest\")[0][0]\n",
    "\n",
    "    attentions = outputs.attentions\n",
    "    print(\"len(attentions):\", len(attentions)) # 24 layers\n",
    "    print(\"attentions[0].shape:\", attentions[0].shape) # [1, 16, 1025, 1025]\n",
    "    # Initialize full attention map\n",
    "    full_attn_map = torch.zeros((1, image_width, image_height)) # (1, 3072, 3072)\n",
    "    masklist = outputs.mask.detach().type(torch.int64) > 0\n",
    "    print(\"masklist.shape:\", masklist.shape) # (1, 4096)\n",
    "    # Determine patch contributions\n",
    "    #patch_contrib = attentions[-1][:, :, 1:, 1:].mean(dim=1) # Take mean attention of last layer (without [CLS] token)\n",
    "    patch_contrib = torch.zeros(attentions[0].shape[-1]-1) # -1 for [CLS] token\n",
    "    check_cls_token = False\n",
    "    if check_cls_token:\n",
    "        for layer_attn in attentions[:-1]:\n",
    "            # Only check [CLS] token\n",
    "            attn = layer_attn[:, :, 1, 1:].detach().cpu() # [1, 16, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1, 1024]\n",
    "            patch_contrib += attn\n",
    "    else:\n",
    "        for layer_attn in attentions[:-1]:\n",
    "            # Remove [CLS] token\n",
    "            attn = layer_attn[:, :, 1:, 1:].detach().cpu() # [1, 16, 1024, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1024, 1024]\n",
    "            # Average contribution across all other tokens\n",
    "            attn = attn.mean(dim=1) # [1024,]\n",
    "            patch_contrib += attn\n",
    "    # Normalize\n",
    "    patch_contrib -= patch_contrib.min()\n",
    "    patch_contrib /= patch_contrib.max()\n",
    "    print(\"patch_contrib.shape:\", patch_contrib.shape)\n",
    "    print(pd.DataFrame(patch_contrib).describe())\n",
    "    # Map attention scores onto full attention map\n",
    "    attn_iter = iter(patch_contrib)\n",
    "    print(\"len(masklist[0]):\", len(masklist[0]))\n",
    "    for i, masked in enumerate(masklist[0]):\n",
    "        row = i // num_patches_x\n",
    "        col = i % num_patches_y\n",
    "        #print(\"i, row, col, masked:\", i, row, col, masked.item())\n",
    "        attn_val = next(attn_iter) if ~masked else torch.tensor([0])\n",
    "        full_attn_map[0, row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size] = attn_val.expand((patch_size, patch_size))\n",
    "    \n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 10]\n",
    "\n",
    "    plt.subplot(1, 5, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 5, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 5, 3)\n",
    "    show_image(y[0], f\"reconstruction (loss: {outputs.loss.item():.4f})\")\n",
    "\n",
    "    plt.subplot(1, 5, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "    \n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(full_attn_map[0], cmap='gray', interpolation='nearest')\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if imgname is not None:\n",
    "        base_path = f\"/home/buehlern/Documents/Masterarbeit/notebooks/Data Exploration Graphics/Model Eval/ViT MAE FT/{mae_name}/\"\n",
    "        Path(base_path).mkdir(exist_ok=True)\n",
    "        plt.savefig(base_path + '/' + str(imgname) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d002e0b4-3ea3-4e81-8260-0d16e80c7d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating shape_to_indices dict in CustomBatchSampler...\n",
      "Done.\n",
      "Maximum bin shape: (2688, 1920)\n",
      "DataLoader length 92\n",
      "Maximum bin shape: (2688, 1152)\n",
      "Generating shape_to_indices dict in CustomBatchSampler...\n",
      "Done.\n",
      "Maximum bin shape: (1920, 1536)\n",
      "Maximum bin shape: (1536, 1152)\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(mri_datamodule.train_dataloader())\n",
    "val_iter = iter(mri_datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4e25-0ada-4aea-b418-c334694b04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(val_iter)\n",
    "print(item[0].shape)\n",
    "visualize(item[0], mae.net, imgname=\"val_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eae25e-20da-43db-91b4-f33fc285f6d9",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97352212-6cc4-4a99-b402-1277af251b36",
   "metadata": {},
   "source": [
    "## Bodypart classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58102e24-6696-472a-8853-e03822a5e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label bodypart as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=1000) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 822\n",
      "Getting val indices...\n",
      "Done. Val len: 38\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 140\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Finetuning DataModule\n",
    "ft_bp_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=\"bodypart\",\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.05,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fca03b-fd49-48d6-b350-d9daa09e7948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1000\n",
       "unique      14\n",
       "top       knee\n",
       "freq       170\n",
       "Name: bodypart, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_bp_datamodule.dsbase.df[\"bodypart\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ab826f-a693-4b92-878f-1925804fecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint path /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../models/configs/experiment/vit_mae_probe_bodypart.yaml')\n",
    "model = MAEFineProbeClassifier(num_classes=cfg.model.num_classes,\n",
    "                                mae_checkpoint=mae_checkpoint,\n",
    "                                optimizer=cfg.model.optimizer,\n",
    "                                scheduler=cfg.model.scheduler,\n",
    "                                seq_mean=cfg.model.seq_mean,\n",
    "                                compile=cfg.model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e275dd-4e22-4d9f-94ad-8ef093673437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 822\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DictConfig' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_bp_datamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:962\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:158\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:138\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:179\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 179\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     rank_zero_warn(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    184\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/models/src/models/vit_mae_probe_module.py:131\u001b[0m, in \u001b[0;36mMAEFineProbeClassifier.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure_optimizers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 131\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mscheduler(optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DictConfig' object is not callable"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloaders=ft_bp_datamodule.train_dataloader())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

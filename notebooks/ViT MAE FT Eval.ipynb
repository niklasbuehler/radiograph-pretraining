{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914d0104-0117-46ba-ab82-5bd4df442dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e71a03-1175-4dda-a061-bc691ae74a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from lightning import Trainer\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054b71d3-d45f-4774-9c0c-19d1f3eb52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/buehlern/Documents/Masterarbeit/models')\n",
    "from src.data.mri_datamodule import MRIDataModule\n",
    "from src.models.vit_mae_module import VisionTransformerMAE\n",
    "from src.models.vit_mae_probe_module import ViTMAELinearProbingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe65309-ec01-4cb5-a65e-637f3cc62c07",
   "metadata": {},
   "source": [
    "# Reconstruction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed73786f-4423-440e-bb41-c37880ee8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "mae_name = \"ViT-L MAE\" # Old pretrained model\n",
    "mae_name = \"ViT-L MAE FT-1\" # FT PT Model normal, 50 epochs\n",
    "mae_name = \"ViT-L MAE FT-2\" # FT PT Model normal, 90 epochs\n",
    "mae_name = \"ViT-L MAE FT-3\" # FT PT Model normal, 300 epochs\n",
    "mae_name = \"ViT-L MAE FT-4\" # FT PT Model normal, 1000 epochs\n",
    "mae_name = \"ViT-L MAE FT-5\" # FT PT Model normal, 10k samples, 3 epochs\n",
    "mae_name = \"ViT-L_MAE_FT-10k_1\" # FT PT Model normal, 10k samples, 10 epochs\n",
    "mae_name = \"ViT-L_MAE_FT-10k_2\" # FT PT Model normal, 10k samples, 10 epochs, 50% mask ratio\n",
    "mae_name = \"ViT-L_MAE_FT-10k_3\" # FT PT Model normal, 10k samples, 10 epochs, patch_size 32\n",
    "# 10k samples, 30 epochs\n",
    "mae_name = \"ViT-L_MAE_FT-10k/default\" # Default FT PT Model, 10k samples, 30 epochs\n",
    "mae_name = \"ViT-L_MAE_FT-10k/downsampling\"\n",
    "mae_name = \"ViT-L_MAE_FT-10k/maskratio\"\n",
    "mae_name = \"ViT-L_MAE_FT-10k/patchsize\"\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1093ed-7315-47f3-bde8-4d9375ffc740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4102834/541953365.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(mae_checkpoint)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(mae_checkpoint)\n",
    "state_dict = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a3eebc-1994-4bf2-9ac5-69c35cf2020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7f43334-7b17-4923-9e0e-579ff102f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(state_dict, unwanted_prefix, new_prefix):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(unwanted_prefix):\n",
    "            new_key = key.replace(unwanted_prefix, new_prefix)\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return collections.OrderedDict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f72f022-1013-4ca2-a2fd-70a13c3b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = remap_keys(state_dict, 'net._orig_mod.', 'net.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6f6955b-1e78-4f20-bad5-5407299affc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc887bf6-fe26-448f-b742-782f5d8aaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = VisionTransformerMAE(image_size = 3072, patch_size = 32, image_channels=1, output_attentions=True)\n",
    "mae.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65a2b4bd-ba0e-4d96-890b-8d7e695159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerMAE(\n",
       "  (net): ViTMAEForPreTraining(\n",
       "    (vit): ViTMAEModel(\n",
       "      (embeddings): ViTMAEEmbeddings(\n",
       "        (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "          (projection): Conv2d(1, 1024, kernel_size=(32, 32), stride=(32, 32))\n",
       "        )\n",
       "      )\n",
       "      (encoder): ViTMAEEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x ViTMAELayer(\n",
       "            (attention): ViTMAEAttention(\n",
       "              (attention): ViTMAESelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTMAESelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTMAEIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTMAEOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): ViTMAEDecoder(\n",
       "      (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (decoder_layers): ModuleList(\n",
       "        (0-7): 8 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (decoder_pred): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_loss): MeanSquaredError()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3757896c-6ad3-4c1d-9f2c-f724bba1f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable masking\n",
    "#mae.net.config.mask_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1352bd16-65db-44fd-b7bd-0b66fc4c7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_attentions: True\n",
      "_attn_implementation: eager\n"
     ]
    }
   ],
   "source": [
    "# For outputting attentions\n",
    "print('output_attentions:', mae.net.config.output_attentions)\n",
    "print('_attn_implementation:', mae.net.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "733f1ff2-9a6a-40ff-a48d-29239a4de188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label bodypart as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_test_114.pkl...\n",
      "MRIDatasetBase(len=114) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 92\n",
      "Getting val indices...\n",
      "Done. Val len: 6\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 16\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the DataModule\n",
    "mri_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_test_114\",\n",
    "    label=\"bodypart\",\n",
    "    batch_binning=\"smart\",\n",
    "    batch_bins=[1152, 1536, 1920, 2304, 2688, 3072],\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.05,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "617949f2-82d8-4ea2-8dd9-20c5f180f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 1]\n",
    "    assert image.shape[2] == 1\n",
    "    plt.imshow(image, cmap=plt.cm.bone)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56503f00-3c36-45f3-9d5b-ffda7674b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pixel_values, model, imgname=None):\n",
    "    patch_size = model.config.patch_size\n",
    "    print(pixel_values.size())\n",
    "    image_width, image_height = pixel_values.size()[-2:]\n",
    "    num_patches_x = image_width // patch_size\n",
    "    num_patches_y = image_height // patch_size\n",
    "    print(f\"Size: {image_width}x{image_height}\")\n",
    "    print(f\"Patches: {num_patches_x}x{num_patches_y}\")\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(pixel_values, interpolate_pos_encoding=True, output_attentions=True)\n",
    "    y = model.unpatchify(outputs.logits, original_image_size=pixel_values.size()[-2:])\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "    \n",
    "    # visualize the mask\n",
    "    mask = outputs.mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 *1)  # (N, H*W, p*p*1)\n",
    "    mask = model.unpatchify(mask, original_image_size=pixel_values.size()[-2:])\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    \n",
    "    x = torch.einsum('nchw->nhwc', pixel_values)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # Attention map calculations\n",
    "    # seq_len = 1025\n",
    "    # Without [CLS] token: seq_len = 1024\n",
    "    # num_patches_x_out = num_patches_y = sqrt(seq_len) = sqrt(1024) = 32\n",
    "    # num_patches_x_in = image_size // patch_size = 3072 // 48 = 64\n",
    "    # Reason for difference is masking! 4096 * (1-0.75) = 1024\n",
    "\n",
    "    #image_size = model.config.image_size\n",
    "    #patch_size = model.config.patch_size\n",
    "    #num_patches = image_size // patch_size\n",
    "    #attn_map = attn.squeeze().numpy()\n",
    "    ## Scale attention map to original image size\n",
    "    #attn_map_scaled = np.zeros((image_size, image_size))\n",
    "    #for i in range(num_patches):\n",
    "    #    for j in range(num_patches):\n",
    "    #        x_start = i * patch_size\n",
    "    #        x_end = x_start + patch_size\n",
    "    #        y_start = j * patch_size\n",
    "    #        y_end = y_start + patch_size\n",
    "    #        \n",
    "    #        attn_map_scaled[x_start:x_end, y_start:y_end] = attn_map[i * num_patches + j]\n",
    "    \n",
    "    #num_patches = attn.size(-1)\n",
    "    #print(f\"num_patches (after masking {model.config.mask_ratio}):\", num_patches) # 1024\n",
    "    #attn = attn[0].reshape(model.config.patch_size, model.config.patch_size, -1)\n",
    "    # Rescale attention to image size\n",
    "    #attn = F.interpolate(attn.unsqueeze(0).unsqueeze(0), scale_factor=model.config.patch_size, mode=\"nearest\")[0][0]\n",
    "\n",
    "    attentions = outputs.attentions\n",
    "    print(\"len(attentions):\", len(attentions)) # 24 layers\n",
    "    print(\"attentions[0].shape:\", attentions[0].shape) # [1, 16, 1025, 1025]\n",
    "    # Initialize full attention map\n",
    "    full_attn_map = torch.zeros((1, image_width, image_height)) # (1, 3072, 3072)\n",
    "    masklist = outputs.mask.detach().type(torch.int64) > 0\n",
    "    print(\"masklist.shape:\", masklist.shape) # (1, 4096)\n",
    "    # Determine patch contributions\n",
    "    #patch_contrib = attentions[-1][:, :, 1:, 1:].mean(dim=1) # Take mean attention of last layer (without [CLS] token)\n",
    "    patch_contrib = torch.zeros(attentions[0].shape[-1]-1) # -1 for [CLS] token\n",
    "    check_cls_token = False\n",
    "    if check_cls_token:\n",
    "        for layer_attn in attentions[:-1]:\n",
    "            # Only check [CLS] token\n",
    "            attn = layer_attn[:, :, 1, 1:].detach().cpu() # [1, 16, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1, 1024]\n",
    "            patch_contrib += attn\n",
    "    else:\n",
    "        for layer_attn in attentions[:-1]:\n",
    "            # Remove [CLS] token\n",
    "            attn = layer_attn[:, :, 1:, 1:].detach().cpu() # [1, 16, 1024, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1024, 1024]\n",
    "            # Average contribution across all other tokens\n",
    "            attn = attn.mean(dim=1) # [1024,]\n",
    "            patch_contrib += attn\n",
    "    # Normalize\n",
    "    patch_contrib -= patch_contrib.min()\n",
    "    patch_contrib /= patch_contrib.max()\n",
    "    print(\"patch_contrib.shape:\", patch_contrib.shape)\n",
    "    print(pd.DataFrame(patch_contrib).describe())\n",
    "    # Map attention scores onto full attention map\n",
    "    attn_iter = iter(patch_contrib)\n",
    "    print(\"len(masklist[0]):\", len(masklist[0]))\n",
    "    for i, masked in enumerate(masklist[0]):\n",
    "        row = i // num_patches_x\n",
    "        col = i % num_patches_y\n",
    "        #print(\"i, row, col, masked:\", i, row, col, masked.item())\n",
    "        attn_val = next(attn_iter) if ~masked else torch.tensor([-1])\n",
    "        full_attn_map[0, row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size] = attn_val.expand((patch_size, patch_size))\n",
    "    \n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 10]\n",
    "\n",
    "    plt.subplot(1, 5, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 5, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 5, 3)\n",
    "    show_image(y[0], f\"reconstruction (loss: {outputs.loss.item():.4f})\")\n",
    "\n",
    "    plt.subplot(1, 5, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "    \n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(full_attn_map[0], cmap='gray', interpolation='nearest')\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if imgname is not None:\n",
    "        base_path = f\"/home/buehlern/Documents/Masterarbeit/notebooks/Data Exploration Graphics/Model Eval/ViT MAE FT/{mae_name}/\"\n",
    "        Path(base_path).mkdir(exist_ok=True)\n",
    "        plt.savefig(base_path + '/' + str(imgname) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d002e0b4-3ea3-4e81-8260-0d16e80c7d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating shape_to_indices dict in CustomBatchSampler...\n",
      "Done.\n",
      "Maximum bin shape: (2688, 1920)\n",
      "DataLoader length 92\n",
      "Maximum bin shape: (2688, 1152)\n",
      "Generating shape_to_indices dict in CustomBatchSampler...\n",
      "Done.\n",
      "Maximum bin shape: (1920, 1536)\n",
      "Maximum bin shape: (1536, 1152)\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(mri_datamodule.train_dataloader())\n",
    "val_iter = iter(mri_datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4e25-0ada-4aea-b418-c334694b04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(val_iter)\n",
    "print(item[0].shape)\n",
    "visualize(item[0], mae.net, imgname=\"val_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eae25e-20da-43db-91b4-f33fc285f6d9",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaaf7ebe-7f48-452b-ab2d-db8a1341c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler_fn(optimizer):\n",
    "    return torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
    "def optim_fn(params):\n",
    "    global lr\n",
    "    return torch.optim.AdamW(params, lr=lr)\n",
    "def run_finetuning(mae_checkpoint, datamodule, num_classes, set_lr, eff_batch_size, num_epochs=1):\n",
    "    global lr\n",
    "    lr = set_lr\n",
    "    grad_acc = eff_batch_size // datamodule.batch_size\n",
    "    print(f\"Testing {mae_checkpoint} on task {datamodule.label} with lr={lr} and effective batch_size={datamodule.batch_size}*{grad_acc}={datamodule.batch_size*grad_acc}\")\n",
    "    ft_probe_model = ViTMAELinearProbingClassifier(\n",
    "        optimizer=optim_fn,\n",
    "        scheduler=scheduler_fn,\n",
    "        mae_checkpoint=mae_checkpoint,\n",
    "        num_classes=num_classes,\n",
    "        freeze_encoder=False,\n",
    "        mean_pooling=True\n",
    "    )\n",
    "    trainer = Trainer(max_epochs=num_epochs, accumulate_grad_batches=grad_acc)\n",
    "    trainer.fit(ft_probe_model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "    return ft_probe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fe3071-1d12-48a2-a419-8597b17b1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5943d-ac88-42cd-b213-417c611d64d7",
   "metadata": {},
   "source": [
    "## Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "813b43fd-d18e-4f05-8ffd-f1a740a53274",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_name = \"ViT-L_MAE_FT-10k/default\" # Default FT PT Model, 10k samples, 30 epochs\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97352212-6cc4-4a99-b402-1277af251b36",
   "metadata": {},
   "source": [
    "### Bodypart classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f3dc1db-8aa5-40d9-b082-9afd6b13ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label bodypart as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=1000) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 781\n",
      "Getting val indices...\n",
      "Done. Val len: 79\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 140\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n",
      "Label bodypart has 14 classes\n"
     ]
    }
   ],
   "source": [
    "label = \"bodypart\"\n",
    "# Bodypart Finetuning DataModule\n",
    "ft_bp_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_bp_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1272409f-24f6-4461-9084-b1f30475a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of fracture in train data: 0.20742637644046094\n",
      "Ratio of fracture in val data: 0.25316455696202533\n",
      "Ratio of fracture in test data: 0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "train_lbl = ft_bp_datamodule.dsbase.df.iloc[ft_bp_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_bp_datamodule.dsbase.df.iloc[ft_bp_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_bp_datamodule.dsbase.df.iloc[ft_bp_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e7672-b20a-4239-859b-0ec988a89f91",
   "metadata": {},
   "source": [
    "#### HParam Search: Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789e925-261e-41b7-a40b-d22bc9b49103",
   "metadata": {},
   "source": [
    "Test learning rates 0.0001, 0.0003, 0.001, 0.003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb2e6bdb-8832-4b2d-9b30-9b56197c40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=1e-05 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4780b9e72747db8e72f9b4c9805c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = ft_probe_model = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.00001, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bda08c90-d859-410c-9736-06262a784a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=3e-05 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac429c0d94645a5b61e321e5b2abec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = ft_probe_model = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.00003, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1436cd4d-3514-4414-8779-8212f95725d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3642c68d1ef461a82a1f658869320e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = ft_probe_model = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.0001, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fdbf755-e5d2-40f4-92f2-701d051e40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.0003 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5137b22c76f64500bb78f6dc7304d6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.0003, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6519c5a-58c4-4d4f-99d8-4e831f2f5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f50da7d1144dce922c50bcd298074d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.001, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "155b269f-5655-4da9-850a-cb113752342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.003 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3b103367ff40b08e1e27643d68b290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.003, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a373f750-5a01-4389-97e7-425db5fe0c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.01 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc69d88ff7914496bbd507ccbf6ea73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.01, effective_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe74a804-2f18-411b-a059-a114c1e95b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.03 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89954bcf8c6c49e691b682ef7c0ba095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "_ = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, 0.03, effective_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2de5dd-2aa1-425b-b414-67ff51906e77",
   "metadata": {},
   "source": [
    "#### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4dfce2-659b-4171-a3af-3a3e1df138d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3e3c23c-bb2f-4af5-b0bc-e84999461dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt on task bodypart with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/default.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6722aa905f4412997f36fd02c25e7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "ft_probe_model = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce2128-61eb-495f-abd2-191ca9c48e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_iter = iter(ft_bp_datamodule.val_dataloader())\n",
    "#item = next(val_iter)\n",
    "#pred = ft_probe_model(item[0])\n",
    "#torch.nn.Softmax(pred)\n",
    "#torch.argmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e970697-8511-4d6e-ac4a-dc7e02d4a941",
   "metadata": {},
   "source": [
    "### Fracture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "418d0330-ea05-4246-ace5-0a32b43b2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label fracture as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=653) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 495\n",
      "Getting val indices...\n",
      "Done. Val len: 57\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 101\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n",
      "Label fracture has 2 classes\n"
     ]
    }
   ],
   "source": [
    "label = \"fracture\"\n",
    "# Fracture Finetuning DataModule\n",
    "ft_frac_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_frac_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b4c7a21-0e50-433a-9825-071d824b48c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of fracture in train data: 0.3111111111111111\n",
      "Ratio of fracture in val data: 0.40350877192982454\n",
      "Ratio of fracture in test data: 0.3465346534653465\n"
     ]
    }
   ],
   "source": [
    "train_lbl = ft_frac_datamodule.dsbase.df.iloc[ft_frac_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_frac_datamodule.dsbase.df.iloc[ft_frac_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_frac_datamodule.dsbase.df.iloc[ft_frac_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da1528ca-dc44-4ebb-86a0-0903348e3fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt on task fracture with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/buehlern/.local/lib/python3.10/site-packages/ ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 2.0 K  | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.481 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4911ffaf0b4e15947399ecf6d88d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_frac_model = run_finetuning(mae_checkpoint, ft_frac_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee76069-2d7c-4274-87ac-83843050ffb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Foreign Material Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd70aeb-91c2-449b-9b1d-807494ca0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"foreignmaterial\"\n",
    "# Fracture Finetuning DataModule\n",
    "ft_fm_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_fm_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172e6d8-79be-4e49-9c70-89630b4812fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1bb27-c649-4c3f-9bba-8e70ec340d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_fm_model = run_finetuning(mae_checkpoint, ft_fm_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275f68f-14c6-409d-9131-42587c54d631",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Downsampling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f70df55d-1c21-463a-a6a9-56a4914640c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_name = \"ViT-L_MAE_FT-10k/downsampling\"\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e74dba-c7e1-4c02-a00c-6a14d6613066",
   "metadata": {},
   "source": [
    "### Bodypart classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13242d59-c4c9-44f0-8e58-2636adbf24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt on task bodypart with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5313dc16f03848eab44ed3e42ed44bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_bp_model_downsampling = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91b090-3302-4b31-9bed-3f5f7a2f8ca6",
   "metadata": {},
   "source": [
    "### Fracture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b2180ed-479d-4b0a-a3f6-dd63444d960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt on task fracture with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/downsampling.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa4f5ed3d624e32b55694a1eaff3f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_frac_model_downsampling = run_finetuning(mae_checkpoint, ft_frac_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed24fb7-de53-48ee-a21f-5af14a2bdc7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Foreign Material Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35072e6f-bc41-4003-9a80-ce57da0af00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"foreignmaterial\"\n",
    "# Fracture Finetuning DataModule\n",
    "ft_fm_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_fm_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ab4e0-cd5c-4225-a7c2-2787fc339ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdfdec-5db9-4748-bd0e-a3b413249e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_frac_model = run_finetuning(mae_checkpoint, ft_fm_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc178d8-bc8a-4eaf-83c4-8cf5f8659e1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Maskratio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f0fcd63-4505-41e4-b5a1-2ea8d0b959e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_name = \"ViT-L_MAE_FT-10k/maskratio\"\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a504e-c482-491d-abfb-22e148defeec",
   "metadata": {},
   "source": [
    "### Bodypart classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aef01233-48b0-4379-9ca9-4977772f6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/maskratio.ckpt on task bodypart with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/maskratio.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3c34bd8f304af8b823556258927c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_bp_model_maskratio = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f375b-f0af-4b68-8270-ce6cdecc8bda",
   "metadata": {},
   "source": [
    "### Fracture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "afa72687-c898-44f7-8808-6c9cabef8550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/maskratio.ckpt on task fracture with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/maskratio.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 308 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "304 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "308 M     Total params\n",
      "1,235.530 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63dd1a88d5542439d04a95d0c7669f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_frac_model_maskratio = run_finetuning(mae_checkpoint, ft_frac_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a1449-d97c-451e-b824-1413d2178f8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Foreign Material Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37a168-2bd9-40a4-91a4-c5ac1cbf16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"foreignmaterial\"\n",
    "# Fracture Finetuning DataModule\n",
    "ft_fm_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=48,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_fm_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe47dd-8bad-40fe-a5e2-a54080a17961",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf39e43-f1ef-444e-90e2-eee43de4a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_frac_model = run_finetuning(mae_checkpoint, ft_fm_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6adc1e-cfce-465f-9357-d3b7247815d5",
   "metadata": {},
   "source": [
    "## Patch Size Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710d89e6-fc78-4f2f-8eed-48188f54b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_name = \"ViT-L_MAE_FT-10k/patchsize\"\n",
    "mae_checkpoint = f\"/home/buehlern/Documents/Masterarbeit/models/checkpoints/{mae_name}.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c82f0-1023-4324-bc3d-9bf37f4d87d4",
   "metadata": {},
   "source": [
    "### Bodypart classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e4521b-c5a2-40a0-843a-3a48413dfe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label bodypart as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=1000) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 781\n",
      "Getting val indices...\n",
      "Done. Val len: 79\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 140\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n",
      "Label bodypart has 14 classes\n"
     ]
    }
   ],
   "source": [
    "label = \"bodypart\"\n",
    "# Bodypart Finetuning DataModule, pad to multiple of new patch_size (32)\n",
    "ft_bp_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=32,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_bp_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3e1d7e-7208-4b1e-b09a-f03e7bf2591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/patchsize.ckpt on task bodypart with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/patchsize.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/buehlern/.local/lib/python3.10/site-packages/ ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 312 M  | train\n",
      "1 | classifier   | Linear               | 14.3 K | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "303 M     Trainable params\n",
      "9.4 M     Non-trainable params\n",
      "312 M     Total params\n",
      "1,251.258 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f21b8d1361948f6a3c92538ca954dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_bp_model_patchsize = run_finetuning(mae_checkpoint, ft_bp_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0f2b4-da18-4d91-b1a5-6bbee03fe2cc",
   "metadata": {},
   "source": [
    "### Fracture Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201f8486-db63-4c1a-b591-a37734e672cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label fracture as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=653) initialized\n",
      "Getting train indices...\n",
      "Done. Train len: 495\n",
      "Getting val indices...\n",
      "Done. Val len: 57\n",
      "Getting test indices...\n",
      "WARN: Including test data\n",
      "Done. Test len: 101\n",
      "Initializing train dataset...\n",
      "Done.\n",
      "Initializing val dataset...\n",
      "Done.\n",
      "Initializing test dataset...\n",
      "Done.\n",
      "Label fracture has 2 classes\n"
     ]
    }
   ],
   "source": [
    "label = \"fracture\"\n",
    "# Fracture Finetuning DataModule, pad to multiple of new patch_size (32)\n",
    "ft_frac_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=32,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_frac_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437301a1-07be-4ef4-95f2-dd1e416a7570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/patchsize.ckpt on task fracture with lr=0.0001 and effective batch_size=1*64=64\n",
      "Loading checkpoint from /home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L_MAE_FT-10k/patchsize.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/buehlern/.local/lib/python3.10/site-packages/ ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                 | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | mae_model    | VisionTransformerMAE | 312 M  | train\n",
      "1 | classifier   | Linear               | 2.0 K  | train\n",
      "2 | criterion    | CrossEntropyLoss     | 0      | train\n",
      "3 | train_acc    | MulticlassAccuracy   | 0      | train\n",
      "4 | val_acc      | MulticlassAccuracy   | 0      | train\n",
      "5 | test_acc     | MulticlassAccuracy   | 0      | train\n",
      "6 | train_loss   | MeanMetric           | 0      | train\n",
      "7 | val_loss     | MeanMetric           | 0      | train\n",
      "8 | test_loss    | MeanMetric           | 0      | train\n",
      "9 | val_acc_best | MaxMetric            | 0      | train\n",
      "--------------------------------------------------------------\n",
      "303 M     Trainable params\n",
      "9.4 M     Non-trainable params\n",
      "312 M     Total params\n",
      "1,251.209 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736ebb0280aa4da7a9ac132e38cfeb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "ft_frac_model_patchsize = run_finetuning(mae_checkpoint, ft_frac_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5eef8-42b6-45ad-9b87-9e896591fec1",
   "metadata": {},
   "source": [
    "### Foreign Material Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d575027-f8b1-4677-8420-1af9e9f4ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label foreignmaterial as stratification_target\n",
      "Initializing MRIDatasetBase...\n",
      "Loading dataframe from /home/buehlern/Documents/Masterarbeit/data/df_min_ft_pt_1k.pkl...\n",
      "MRIDatasetBase(len=653) initialized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WARN: NO TRAINVAL TEST SPLIT FOUND AT /home/buehlern/Documents/Masterarbeit/data/splits/split_test_df_min_ft_pt_1k_straton_foreignmaterial.csv, type YES[enter] to generate one:  YES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: GENERATING NEW TRAINVAL TEST SPLIT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeignmaterial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Fracture Finetuning DataModule, pad to multiple of new patch_size (32)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ft_fm_datamodule \u001b[38;5;241m=\u001b[39m \u001b[43mMRIDataModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdf_min_ft_pt_1k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersistent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_inverted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m ft_fm_datamodule\u001b[38;5;241m.\u001b[39mdsbase\u001b[38;5;241m.\u001b[39mdf[label]\u001b[38;5;241m.\u001b[39mdescribe()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/models/src/data/mri_datamodule.py:71\u001b[0m, in \u001b[0;36mMRIDataModule.__init__\u001b[0;34m(self, batch_size, num_workers, persistent_workers, pin_memory, basedir, df_name, max_size_padoutside, pad_to_multiple_of, batch_binning, batch_bins, normalization_mode, downsampling, output_channels, total_data_size, fix_inverted, label, stratification_target, val_size, test_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_val: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_test: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/models/src/data/mri_datamodule.py:93\u001b[0m, in \u001b[0;36mMRIDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_val \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_test:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m---> 93\u001b[0m     split_test_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trainval_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting train indices...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m     train_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_split_df(split_test_loc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/models/src/data/mri_datamodule.py:133\u001b[0m, in \u001b[0;36mMRIDataModule.generate_trainval_test_split\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWARN: GENERATING NEW TRAINVAL TEST SPLIT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    129\u001b[0m patientid_to_strattarget \u001b[38;5;241m=\u001b[39m {patientid: \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(subdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstratification_target]),\n\u001b[1;32m    130\u001b[0m                                               key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: stratification_target_frequencies\u001b[38;5;241m.\u001b[39mloc[x])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m patientid, subdf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdsbase\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatientid\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[0;32m--> 133\u001b[0m _, test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatientid_to_strattarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatientid_to_strattarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m test_patientids \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test)\n\u001b[1;32m    136\u001b[0m test_patientids\u001b[38;5;241m.\u001b[39mto_csv(split_test_loc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2801\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2797\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2799\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2807\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2808\u001b[0m     )\n\u001b[1;32m   2809\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1843\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m \n\u001b[1;32m   1815\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1842\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1843\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2247\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2245\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2258\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "label = \"foreignmaterial\"\n",
    "# Fracture Finetuning DataModule, pad to multiple of new patch_size (32)\n",
    "ft_fm_datamodule = MRIDataModule(\n",
    "    df_name=\"df_min_ft_pt_1k\",\n",
    "    label=label,\n",
    "    pad_to_multiple_of=32,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    output_channels=1,\n",
    "    fix_inverted=True\n",
    ")\n",
    "num_classes = ft_fm_datamodule.dsbase.df[label].describe()[\"unique\"]\n",
    "print(f\"Label {label} has {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64850b05-80ad-4e20-88aa-c02c9c4780cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_train.indices][\"fracture_bool\"]\n",
    "val_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_val.indices][\"fracture_bool\"]\n",
    "test_lbl = ft_fm_datamodule.dsbase.df.iloc[ft_fm_datamodule.data_test.indices][\"fracture_bool\"]\n",
    "print(f\"Ratio of fracture in train data:\", train_lbl.sum() / len(train_lbl))\n",
    "print(f\"Ratio of fracture in val data:\", val_lbl.sum() / len(val_lbl))\n",
    "print(f\"Ratio of fracture in test data:\", test_lbl.sum() / len(test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6da25-6471-49bf-84b9-e31c73568886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_frac_model = run_finetuning(mae_checkpoint, ft_fm_datamodule, num_classes, best_lr, effective_batch_size, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

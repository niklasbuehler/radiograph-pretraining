{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e71a03-1175-4dda-a061-bc691ae74a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054b71d3-d45f-4774-9c0c-19d1f3eb52a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/buehlern/Documents/Masterarbeit/models')\n",
    "from src.data.mri_datamodule import MRIDataModule\n",
    "from src.models.vit_mae_module import VisionTransformerMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed73786f-4423-440e-bb41-c37880ee8672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1513142/2628517626.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(mae_checkpoint)\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "mae_checkpoint = \"/home/buehlern/Documents/Masterarbeit/models/logs/train/multiruns/2024-07-29_12-12-24/0/checkpoints/epoch_009.ckpt\"\n",
    "checkpoint = torch.load(mae_checkpoint)\n",
    "state_dict = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a3eebc-1994-4bf2-9ac5-69c35cf2020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net._orig_mod.vit.embeddings.cls_token', 'net._orig_mod.vit.embeddings.position_embeddings', 'net._orig_mod.vit.embeddings.patch_embeddings.projection.weight', 'net._orig_mod.vit.embeddings.patch_embeddings.projection.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.0.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.0.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.0.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.0.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.0.output.dense.weight', 'net._orig_mod.vit.encoder.layer.0.output.dense.bias', 'net._orig_mod.vit.encoder.layer.0.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.0.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.0.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.0.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.1.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.1.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.1.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.1.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.1.output.dense.weight', 'net._orig_mod.vit.encoder.layer.1.output.dense.bias', 'net._orig_mod.vit.encoder.layer.1.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.1.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.1.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.1.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.2.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.2.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.2.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.2.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.2.output.dense.weight', 'net._orig_mod.vit.encoder.layer.2.output.dense.bias', 'net._orig_mod.vit.encoder.layer.2.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.2.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.2.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.2.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.3.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.3.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.3.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.3.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.3.output.dense.weight', 'net._orig_mod.vit.encoder.layer.3.output.dense.bias', 'net._orig_mod.vit.encoder.layer.3.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.3.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.3.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.3.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.4.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.4.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.4.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.4.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.4.output.dense.weight', 'net._orig_mod.vit.encoder.layer.4.output.dense.bias', 'net._orig_mod.vit.encoder.layer.4.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.4.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.4.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.4.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.5.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.5.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.5.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.5.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.5.output.dense.weight', 'net._orig_mod.vit.encoder.layer.5.output.dense.bias', 'net._orig_mod.vit.encoder.layer.5.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.5.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.5.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.5.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.6.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.6.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.6.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.6.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.6.output.dense.weight', 'net._orig_mod.vit.encoder.layer.6.output.dense.bias', 'net._orig_mod.vit.encoder.layer.6.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.6.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.6.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.6.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.7.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.7.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.7.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.7.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.7.output.dense.weight', 'net._orig_mod.vit.encoder.layer.7.output.dense.bias', 'net._orig_mod.vit.encoder.layer.7.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.7.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.7.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.7.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.8.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.8.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.8.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.8.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.8.output.dense.weight', 'net._orig_mod.vit.encoder.layer.8.output.dense.bias', 'net._orig_mod.vit.encoder.layer.8.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.8.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.8.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.8.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.9.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.9.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.9.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.9.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.9.output.dense.weight', 'net._orig_mod.vit.encoder.layer.9.output.dense.bias', 'net._orig_mod.vit.encoder.layer.9.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.9.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.9.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.9.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.10.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.10.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.10.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.10.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.10.output.dense.weight', 'net._orig_mod.vit.encoder.layer.10.output.dense.bias', 'net._orig_mod.vit.encoder.layer.10.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.10.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.10.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.10.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.11.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.11.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.11.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.11.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.11.output.dense.weight', 'net._orig_mod.vit.encoder.layer.11.output.dense.bias', 'net._orig_mod.vit.encoder.layer.11.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.11.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.11.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.11.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.12.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.12.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.12.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.12.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.12.output.dense.weight', 'net._orig_mod.vit.encoder.layer.12.output.dense.bias', 'net._orig_mod.vit.encoder.layer.12.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.12.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.12.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.12.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.13.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.13.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.13.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.13.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.13.output.dense.weight', 'net._orig_mod.vit.encoder.layer.13.output.dense.bias', 'net._orig_mod.vit.encoder.layer.13.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.13.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.13.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.13.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.14.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.14.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.14.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.14.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.14.output.dense.weight', 'net._orig_mod.vit.encoder.layer.14.output.dense.bias', 'net._orig_mod.vit.encoder.layer.14.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.14.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.14.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.14.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.15.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.15.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.15.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.15.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.15.output.dense.weight', 'net._orig_mod.vit.encoder.layer.15.output.dense.bias', 'net._orig_mod.vit.encoder.layer.15.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.15.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.15.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.15.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.16.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.16.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.16.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.16.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.16.output.dense.weight', 'net._orig_mod.vit.encoder.layer.16.output.dense.bias', 'net._orig_mod.vit.encoder.layer.16.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.16.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.16.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.16.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.17.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.17.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.17.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.17.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.17.output.dense.weight', 'net._orig_mod.vit.encoder.layer.17.output.dense.bias', 'net._orig_mod.vit.encoder.layer.17.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.17.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.17.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.17.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.18.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.18.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.18.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.18.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.18.output.dense.weight', 'net._orig_mod.vit.encoder.layer.18.output.dense.bias', 'net._orig_mod.vit.encoder.layer.18.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.18.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.18.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.18.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.19.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.19.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.19.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.19.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.19.output.dense.weight', 'net._orig_mod.vit.encoder.layer.19.output.dense.bias', 'net._orig_mod.vit.encoder.layer.19.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.19.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.19.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.19.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.20.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.20.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.20.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.20.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.20.output.dense.weight', 'net._orig_mod.vit.encoder.layer.20.output.dense.bias', 'net._orig_mod.vit.encoder.layer.20.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.20.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.20.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.20.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.21.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.21.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.21.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.21.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.21.output.dense.weight', 'net._orig_mod.vit.encoder.layer.21.output.dense.bias', 'net._orig_mod.vit.encoder.layer.21.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.21.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.21.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.21.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.22.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.22.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.22.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.22.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.22.output.dense.weight', 'net._orig_mod.vit.encoder.layer.22.output.dense.bias', 'net._orig_mod.vit.encoder.layer.22.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.22.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.22.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.22.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.23.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.23.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.23.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.23.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.23.output.dense.weight', 'net._orig_mod.vit.encoder.layer.23.output.dense.bias', 'net._orig_mod.vit.encoder.layer.23.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.23.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.23.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.23.layernorm_after.bias', 'net._orig_mod.vit.layernorm.weight', 'net._orig_mod.vit.layernorm.bias', 'net._orig_mod.decoder.mask_token', 'net._orig_mod.decoder.decoder_pos_embed', 'net._orig_mod.decoder.decoder_embed.weight', 'net._orig_mod.decoder.decoder_embed.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.0.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.0.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.0.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.1.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.1.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.1.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.2.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.2.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.2.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.3.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.3.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.3.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.4.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.4.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.4.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.5.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.5.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.5.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.6.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.6.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.6.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.7.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.7.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.7.layernorm_after.bias', 'net._orig_mod.decoder.decoder_norm.weight', 'net._orig_mod.decoder.decoder_norm.bias', 'net._orig_mod.decoder.decoder_pred.weight', 'net._orig_mod.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f43334-7b17-4923-9e0e-579ff102f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(state_dict, unwanted_prefix, new_prefix):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(unwanted_prefix):\n",
    "            new_key = key.replace(unwanted_prefix, new_prefix)\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return collections.OrderedDict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f72f022-1013-4ca2-a2fd-70a13c3b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = remap_keys(state_dict, 'net._orig_mod.', 'net.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f6955b-1e78-4f20-bad5-5407299affc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc887bf6-fe26-448f-b742-782f5d8aaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = VisionTransformerMAE(image_size = 3072, patch_size = 48)\n",
    "mae.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a2b4bd-ba0e-4d96-890b-8d7e695159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerMAE(\n",
       "  (net): ViTMAEForPreTraining(\n",
       "    (vit): ViTMAEModel(\n",
       "      (embeddings): ViTMAEEmbeddings(\n",
       "        (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "          (projection): Conv2d(1, 768, kernel_size=(48, 48), stride=(48, 48))\n",
       "        )\n",
       "      )\n",
       "      (encoder): ViTMAEEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x ViTMAELayer(\n",
       "            (attention): ViTMAESdpaAttention(\n",
       "              (attention): ViTMAESdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTMAESelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTMAEIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTMAEOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): ViTMAEDecoder(\n",
       "      (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (decoder_layers): ModuleList(\n",
       "        (0-7): 8 x ViTMAELayer(\n",
       "          (attention): ViTMAESdpaAttention(\n",
       "            (attention): ViTMAESdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (decoder_pred): Linear(in_features=512, out_features=2304, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_loss): MeanSquaredError()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e2a7c9-146c-4b56-9706-c5a37d6d3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable masking\n",
    "#mae.net.config.mask_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "733f1ff2-9a6a-40ff-a48d-29239a4de188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing MRIDatasetBase ...\n",
      "reading /home/buehlern/Documents/Masterarbeit/data/clean_df_slim.pkl file ...\n",
      "PATH /home/buehlern/Documents/Masterarbeit/data/BodyPartExamined_mappings_mergemore.json\n",
      "/home/buehlern/Documents/Masterarbeit/data/cache-full/df_labelcomparison.pkl does not exit --> no items excluded by it\n",
      "MRIDatasetBase(len=639877) initialized\n",
      "\n",
      "initializing MRIDataset(mode=train) ...\n",
      "MRIDataset(mode=train, len=516402) initialized\n",
      "\n",
      "initializing MRIDataset(mode=val) ...\n",
      "MRIDataset(mode=val, len=27518) initialized\n",
      "\n",
      "initializing MRIDataset(mode=test) ...\n",
      "WARN: including test data\n",
      "MRIDataset(mode=test, len=95957) initialized\n"
     ]
    }
   ],
   "source": [
    "# Load the DataModule\n",
    "mri_datamodule = MRIDataModule(image_size = 3072, square = True, output_channels = 1, cache = False, fix_inverted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617949f2-82d8-4ea2-8dd9-20c5f180f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 1]\n",
    "    assert image.shape[2] == 1\n",
    "    plt.imshow(image, cmap=plt.cm.bone)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56503f00-3c36-45f3-9d5b-ffda7674b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pixel_values, model, imgname=None):\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values)\n",
    "    y = model.unpatchify(outputs.logits)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "    \n",
    "    # visualize the mask\n",
    "    mask = outputs.mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 *1)  # (N, H*W, p*p*1)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    \n",
    "    x = torch.einsum('nchw->nhwc', pixel_values)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 10]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0], f\"reconstruction (loss: {outputs.loss.item():.4f})\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    if imgname is not None:\n",
    "        plt.savefig('/home/buehlern/Documents/Masterarbeit/notebooks/Data Exploration Graphics/Model Eval/ViT MAE/' + str(imgname) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002e0b4-3ea3-4e81-8260-0d16e80c7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = iter(mri_datamodule.data_val)\n",
    "shuf_dl_iter = iter(random.sample(list(mri_datamodule.data_val), len(mri_datamodule.data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58102e24-6696-472a-8853-e03822a5e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    item = next(shuf_dl_iter)\n",
    "    image = item[0]\n",
    "    batch = image.unsqueeze(0)\n",
    "    visualize(batch, mae.net, imgname=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

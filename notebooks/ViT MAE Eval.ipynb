{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e71a03-1175-4dda-a061-bc691ae74a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054b71d3-d45f-4774-9c0c-19d1f3eb52a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buehlern/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/buehlern/Documents/Masterarbeit/models')\n",
    "from src.data.mri_datamodule import MRIDataModule\n",
    "from src.models.vit_mae_module import VisionTransformerMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed73786f-4423-440e-bb41-c37880ee8672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_638206/1545113530.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(mae_checkpoint)\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/buehlern/.local/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "# Load model checkpoint\n",
    "mae_checkpoint = \"/home/buehlern/Documents/Masterarbeit/models/checkpoints/ViT-L MAE.ckpt\"\n",
    "checkpoint = torch.load(mae_checkpoint)\n",
    "state_dict = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a3eebc-1994-4bf2-9ac5-69c35cf2020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net._orig_mod.vit.embeddings.cls_token', 'net._orig_mod.vit.embeddings.position_embeddings', 'net._orig_mod.vit.embeddings.patch_embeddings.projection.weight', 'net._orig_mod.vit.embeddings.patch_embeddings.projection.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.0.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.0.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.0.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.0.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.0.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.0.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.0.output.dense.weight', 'net._orig_mod.vit.encoder.layer.0.output.dense.bias', 'net._orig_mod.vit.encoder.layer.0.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.0.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.0.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.0.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.1.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.1.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.1.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.1.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.1.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.1.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.1.output.dense.weight', 'net._orig_mod.vit.encoder.layer.1.output.dense.bias', 'net._orig_mod.vit.encoder.layer.1.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.1.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.1.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.1.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.2.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.2.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.2.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.2.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.2.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.2.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.2.output.dense.weight', 'net._orig_mod.vit.encoder.layer.2.output.dense.bias', 'net._orig_mod.vit.encoder.layer.2.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.2.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.2.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.2.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.3.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.3.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.3.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.3.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.3.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.3.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.3.output.dense.weight', 'net._orig_mod.vit.encoder.layer.3.output.dense.bias', 'net._orig_mod.vit.encoder.layer.3.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.3.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.3.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.3.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.4.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.4.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.4.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.4.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.4.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.4.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.4.output.dense.weight', 'net._orig_mod.vit.encoder.layer.4.output.dense.bias', 'net._orig_mod.vit.encoder.layer.4.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.4.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.4.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.4.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.5.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.5.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.5.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.5.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.5.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.5.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.5.output.dense.weight', 'net._orig_mod.vit.encoder.layer.5.output.dense.bias', 'net._orig_mod.vit.encoder.layer.5.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.5.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.5.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.5.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.6.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.6.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.6.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.6.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.6.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.6.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.6.output.dense.weight', 'net._orig_mod.vit.encoder.layer.6.output.dense.bias', 'net._orig_mod.vit.encoder.layer.6.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.6.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.6.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.6.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.7.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.7.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.7.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.7.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.7.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.7.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.7.output.dense.weight', 'net._orig_mod.vit.encoder.layer.7.output.dense.bias', 'net._orig_mod.vit.encoder.layer.7.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.7.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.7.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.7.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.8.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.8.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.8.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.8.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.8.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.8.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.8.output.dense.weight', 'net._orig_mod.vit.encoder.layer.8.output.dense.bias', 'net._orig_mod.vit.encoder.layer.8.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.8.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.8.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.8.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.9.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.9.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.9.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.9.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.9.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.9.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.9.output.dense.weight', 'net._orig_mod.vit.encoder.layer.9.output.dense.bias', 'net._orig_mod.vit.encoder.layer.9.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.9.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.9.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.9.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.10.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.10.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.10.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.10.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.10.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.10.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.10.output.dense.weight', 'net._orig_mod.vit.encoder.layer.10.output.dense.bias', 'net._orig_mod.vit.encoder.layer.10.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.10.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.10.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.10.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.11.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.11.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.11.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.11.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.11.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.11.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.11.output.dense.weight', 'net._orig_mod.vit.encoder.layer.11.output.dense.bias', 'net._orig_mod.vit.encoder.layer.11.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.11.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.11.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.11.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.12.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.12.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.12.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.12.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.12.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.12.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.12.output.dense.weight', 'net._orig_mod.vit.encoder.layer.12.output.dense.bias', 'net._orig_mod.vit.encoder.layer.12.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.12.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.12.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.12.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.13.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.13.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.13.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.13.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.13.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.13.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.13.output.dense.weight', 'net._orig_mod.vit.encoder.layer.13.output.dense.bias', 'net._orig_mod.vit.encoder.layer.13.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.13.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.13.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.13.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.14.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.14.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.14.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.14.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.14.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.14.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.14.output.dense.weight', 'net._orig_mod.vit.encoder.layer.14.output.dense.bias', 'net._orig_mod.vit.encoder.layer.14.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.14.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.14.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.14.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.15.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.15.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.15.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.15.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.15.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.15.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.15.output.dense.weight', 'net._orig_mod.vit.encoder.layer.15.output.dense.bias', 'net._orig_mod.vit.encoder.layer.15.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.15.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.15.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.15.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.16.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.16.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.16.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.16.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.16.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.16.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.16.output.dense.weight', 'net._orig_mod.vit.encoder.layer.16.output.dense.bias', 'net._orig_mod.vit.encoder.layer.16.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.16.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.16.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.16.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.17.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.17.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.17.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.17.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.17.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.17.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.17.output.dense.weight', 'net._orig_mod.vit.encoder.layer.17.output.dense.bias', 'net._orig_mod.vit.encoder.layer.17.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.17.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.17.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.17.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.18.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.18.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.18.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.18.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.18.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.18.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.18.output.dense.weight', 'net._orig_mod.vit.encoder.layer.18.output.dense.bias', 'net._orig_mod.vit.encoder.layer.18.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.18.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.18.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.18.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.19.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.19.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.19.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.19.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.19.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.19.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.19.output.dense.weight', 'net._orig_mod.vit.encoder.layer.19.output.dense.bias', 'net._orig_mod.vit.encoder.layer.19.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.19.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.19.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.19.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.20.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.20.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.20.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.20.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.20.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.20.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.20.output.dense.weight', 'net._orig_mod.vit.encoder.layer.20.output.dense.bias', 'net._orig_mod.vit.encoder.layer.20.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.20.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.20.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.20.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.21.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.21.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.21.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.21.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.21.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.21.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.21.output.dense.weight', 'net._orig_mod.vit.encoder.layer.21.output.dense.bias', 'net._orig_mod.vit.encoder.layer.21.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.21.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.21.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.21.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.22.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.22.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.22.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.22.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.22.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.22.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.22.output.dense.weight', 'net._orig_mod.vit.encoder.layer.22.output.dense.bias', 'net._orig_mod.vit.encoder.layer.22.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.22.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.22.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.22.layernorm_after.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.query.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.query.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.key.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.key.bias', 'net._orig_mod.vit.encoder.layer.23.attention.attention.value.weight', 'net._orig_mod.vit.encoder.layer.23.attention.attention.value.bias', 'net._orig_mod.vit.encoder.layer.23.attention.output.dense.weight', 'net._orig_mod.vit.encoder.layer.23.attention.output.dense.bias', 'net._orig_mod.vit.encoder.layer.23.intermediate.dense.weight', 'net._orig_mod.vit.encoder.layer.23.intermediate.dense.bias', 'net._orig_mod.vit.encoder.layer.23.output.dense.weight', 'net._orig_mod.vit.encoder.layer.23.output.dense.bias', 'net._orig_mod.vit.encoder.layer.23.layernorm_before.weight', 'net._orig_mod.vit.encoder.layer.23.layernorm_before.bias', 'net._orig_mod.vit.encoder.layer.23.layernorm_after.weight', 'net._orig_mod.vit.encoder.layer.23.layernorm_after.bias', 'net._orig_mod.vit.layernorm.weight', 'net._orig_mod.vit.layernorm.bias', 'net._orig_mod.decoder.mask_token', 'net._orig_mod.decoder.decoder_pos_embed', 'net._orig_mod.decoder.decoder_embed.weight', 'net._orig_mod.decoder.decoder_embed.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.0.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.0.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.0.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.0.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.0.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.0.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.1.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.1.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.1.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.1.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.1.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.1.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.2.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.2.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.2.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.2.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.2.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.2.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.3.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.3.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.3.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.3.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.3.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.3.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.4.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.4.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.4.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.4.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.4.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.4.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.5.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.5.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.5.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.5.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.5.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.5.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.6.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.6.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.6.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.6.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.6.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.6.layernorm_after.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.query.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.query.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.key.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.key.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.value.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.attention.value.bias', 'net._orig_mod.decoder.decoder_layers.7.attention.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.attention.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.intermediate.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.intermediate.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.output.dense.weight', 'net._orig_mod.decoder.decoder_layers.7.output.dense.bias', 'net._orig_mod.decoder.decoder_layers.7.layernorm_before.weight', 'net._orig_mod.decoder.decoder_layers.7.layernorm_before.bias', 'net._orig_mod.decoder.decoder_layers.7.layernorm_after.weight', 'net._orig_mod.decoder.decoder_layers.7.layernorm_after.bias', 'net._orig_mod.decoder.decoder_norm.weight', 'net._orig_mod.decoder.decoder_norm.bias', 'net._orig_mod.decoder.decoder_pred.weight', 'net._orig_mod.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f43334-7b17-4923-9e0e-579ff102f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(state_dict, unwanted_prefix, new_prefix):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(unwanted_prefix):\n",
    "            new_key = key.replace(unwanted_prefix, new_prefix)\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return collections.OrderedDict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f72f022-1013-4ca2-a2fd-70a13c3b888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = remap_keys(state_dict, 'net._orig_mod.', 'net.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f6955b-1e78-4f20-bad5-5407299affc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net.vit.embeddings.cls_token', 'net.vit.embeddings.position_embeddings', 'net.vit.embeddings.patch_embeddings.projection.weight', 'net.vit.embeddings.patch_embeddings.projection.bias', 'net.vit.encoder.layer.0.attention.attention.query.weight', 'net.vit.encoder.layer.0.attention.attention.query.bias', 'net.vit.encoder.layer.0.attention.attention.key.weight', 'net.vit.encoder.layer.0.attention.attention.key.bias', 'net.vit.encoder.layer.0.attention.attention.value.weight', 'net.vit.encoder.layer.0.attention.attention.value.bias', 'net.vit.encoder.layer.0.attention.output.dense.weight', 'net.vit.encoder.layer.0.attention.output.dense.bias', 'net.vit.encoder.layer.0.intermediate.dense.weight', 'net.vit.encoder.layer.0.intermediate.dense.bias', 'net.vit.encoder.layer.0.output.dense.weight', 'net.vit.encoder.layer.0.output.dense.bias', 'net.vit.encoder.layer.0.layernorm_before.weight', 'net.vit.encoder.layer.0.layernorm_before.bias', 'net.vit.encoder.layer.0.layernorm_after.weight', 'net.vit.encoder.layer.0.layernorm_after.bias', 'net.vit.encoder.layer.1.attention.attention.query.weight', 'net.vit.encoder.layer.1.attention.attention.query.bias', 'net.vit.encoder.layer.1.attention.attention.key.weight', 'net.vit.encoder.layer.1.attention.attention.key.bias', 'net.vit.encoder.layer.1.attention.attention.value.weight', 'net.vit.encoder.layer.1.attention.attention.value.bias', 'net.vit.encoder.layer.1.attention.output.dense.weight', 'net.vit.encoder.layer.1.attention.output.dense.bias', 'net.vit.encoder.layer.1.intermediate.dense.weight', 'net.vit.encoder.layer.1.intermediate.dense.bias', 'net.vit.encoder.layer.1.output.dense.weight', 'net.vit.encoder.layer.1.output.dense.bias', 'net.vit.encoder.layer.1.layernorm_before.weight', 'net.vit.encoder.layer.1.layernorm_before.bias', 'net.vit.encoder.layer.1.layernorm_after.weight', 'net.vit.encoder.layer.1.layernorm_after.bias', 'net.vit.encoder.layer.2.attention.attention.query.weight', 'net.vit.encoder.layer.2.attention.attention.query.bias', 'net.vit.encoder.layer.2.attention.attention.key.weight', 'net.vit.encoder.layer.2.attention.attention.key.bias', 'net.vit.encoder.layer.2.attention.attention.value.weight', 'net.vit.encoder.layer.2.attention.attention.value.bias', 'net.vit.encoder.layer.2.attention.output.dense.weight', 'net.vit.encoder.layer.2.attention.output.dense.bias', 'net.vit.encoder.layer.2.intermediate.dense.weight', 'net.vit.encoder.layer.2.intermediate.dense.bias', 'net.vit.encoder.layer.2.output.dense.weight', 'net.vit.encoder.layer.2.output.dense.bias', 'net.vit.encoder.layer.2.layernorm_before.weight', 'net.vit.encoder.layer.2.layernorm_before.bias', 'net.vit.encoder.layer.2.layernorm_after.weight', 'net.vit.encoder.layer.2.layernorm_after.bias', 'net.vit.encoder.layer.3.attention.attention.query.weight', 'net.vit.encoder.layer.3.attention.attention.query.bias', 'net.vit.encoder.layer.3.attention.attention.key.weight', 'net.vit.encoder.layer.3.attention.attention.key.bias', 'net.vit.encoder.layer.3.attention.attention.value.weight', 'net.vit.encoder.layer.3.attention.attention.value.bias', 'net.vit.encoder.layer.3.attention.output.dense.weight', 'net.vit.encoder.layer.3.attention.output.dense.bias', 'net.vit.encoder.layer.3.intermediate.dense.weight', 'net.vit.encoder.layer.3.intermediate.dense.bias', 'net.vit.encoder.layer.3.output.dense.weight', 'net.vit.encoder.layer.3.output.dense.bias', 'net.vit.encoder.layer.3.layernorm_before.weight', 'net.vit.encoder.layer.3.layernorm_before.bias', 'net.vit.encoder.layer.3.layernorm_after.weight', 'net.vit.encoder.layer.3.layernorm_after.bias', 'net.vit.encoder.layer.4.attention.attention.query.weight', 'net.vit.encoder.layer.4.attention.attention.query.bias', 'net.vit.encoder.layer.4.attention.attention.key.weight', 'net.vit.encoder.layer.4.attention.attention.key.bias', 'net.vit.encoder.layer.4.attention.attention.value.weight', 'net.vit.encoder.layer.4.attention.attention.value.bias', 'net.vit.encoder.layer.4.attention.output.dense.weight', 'net.vit.encoder.layer.4.attention.output.dense.bias', 'net.vit.encoder.layer.4.intermediate.dense.weight', 'net.vit.encoder.layer.4.intermediate.dense.bias', 'net.vit.encoder.layer.4.output.dense.weight', 'net.vit.encoder.layer.4.output.dense.bias', 'net.vit.encoder.layer.4.layernorm_before.weight', 'net.vit.encoder.layer.4.layernorm_before.bias', 'net.vit.encoder.layer.4.layernorm_after.weight', 'net.vit.encoder.layer.4.layernorm_after.bias', 'net.vit.encoder.layer.5.attention.attention.query.weight', 'net.vit.encoder.layer.5.attention.attention.query.bias', 'net.vit.encoder.layer.5.attention.attention.key.weight', 'net.vit.encoder.layer.5.attention.attention.key.bias', 'net.vit.encoder.layer.5.attention.attention.value.weight', 'net.vit.encoder.layer.5.attention.attention.value.bias', 'net.vit.encoder.layer.5.attention.output.dense.weight', 'net.vit.encoder.layer.5.attention.output.dense.bias', 'net.vit.encoder.layer.5.intermediate.dense.weight', 'net.vit.encoder.layer.5.intermediate.dense.bias', 'net.vit.encoder.layer.5.output.dense.weight', 'net.vit.encoder.layer.5.output.dense.bias', 'net.vit.encoder.layer.5.layernorm_before.weight', 'net.vit.encoder.layer.5.layernorm_before.bias', 'net.vit.encoder.layer.5.layernorm_after.weight', 'net.vit.encoder.layer.5.layernorm_after.bias', 'net.vit.encoder.layer.6.attention.attention.query.weight', 'net.vit.encoder.layer.6.attention.attention.query.bias', 'net.vit.encoder.layer.6.attention.attention.key.weight', 'net.vit.encoder.layer.6.attention.attention.key.bias', 'net.vit.encoder.layer.6.attention.attention.value.weight', 'net.vit.encoder.layer.6.attention.attention.value.bias', 'net.vit.encoder.layer.6.attention.output.dense.weight', 'net.vit.encoder.layer.6.attention.output.dense.bias', 'net.vit.encoder.layer.6.intermediate.dense.weight', 'net.vit.encoder.layer.6.intermediate.dense.bias', 'net.vit.encoder.layer.6.output.dense.weight', 'net.vit.encoder.layer.6.output.dense.bias', 'net.vit.encoder.layer.6.layernorm_before.weight', 'net.vit.encoder.layer.6.layernorm_before.bias', 'net.vit.encoder.layer.6.layernorm_after.weight', 'net.vit.encoder.layer.6.layernorm_after.bias', 'net.vit.encoder.layer.7.attention.attention.query.weight', 'net.vit.encoder.layer.7.attention.attention.query.bias', 'net.vit.encoder.layer.7.attention.attention.key.weight', 'net.vit.encoder.layer.7.attention.attention.key.bias', 'net.vit.encoder.layer.7.attention.attention.value.weight', 'net.vit.encoder.layer.7.attention.attention.value.bias', 'net.vit.encoder.layer.7.attention.output.dense.weight', 'net.vit.encoder.layer.7.attention.output.dense.bias', 'net.vit.encoder.layer.7.intermediate.dense.weight', 'net.vit.encoder.layer.7.intermediate.dense.bias', 'net.vit.encoder.layer.7.output.dense.weight', 'net.vit.encoder.layer.7.output.dense.bias', 'net.vit.encoder.layer.7.layernorm_before.weight', 'net.vit.encoder.layer.7.layernorm_before.bias', 'net.vit.encoder.layer.7.layernorm_after.weight', 'net.vit.encoder.layer.7.layernorm_after.bias', 'net.vit.encoder.layer.8.attention.attention.query.weight', 'net.vit.encoder.layer.8.attention.attention.query.bias', 'net.vit.encoder.layer.8.attention.attention.key.weight', 'net.vit.encoder.layer.8.attention.attention.key.bias', 'net.vit.encoder.layer.8.attention.attention.value.weight', 'net.vit.encoder.layer.8.attention.attention.value.bias', 'net.vit.encoder.layer.8.attention.output.dense.weight', 'net.vit.encoder.layer.8.attention.output.dense.bias', 'net.vit.encoder.layer.8.intermediate.dense.weight', 'net.vit.encoder.layer.8.intermediate.dense.bias', 'net.vit.encoder.layer.8.output.dense.weight', 'net.vit.encoder.layer.8.output.dense.bias', 'net.vit.encoder.layer.8.layernorm_before.weight', 'net.vit.encoder.layer.8.layernorm_before.bias', 'net.vit.encoder.layer.8.layernorm_after.weight', 'net.vit.encoder.layer.8.layernorm_after.bias', 'net.vit.encoder.layer.9.attention.attention.query.weight', 'net.vit.encoder.layer.9.attention.attention.query.bias', 'net.vit.encoder.layer.9.attention.attention.key.weight', 'net.vit.encoder.layer.9.attention.attention.key.bias', 'net.vit.encoder.layer.9.attention.attention.value.weight', 'net.vit.encoder.layer.9.attention.attention.value.bias', 'net.vit.encoder.layer.9.attention.output.dense.weight', 'net.vit.encoder.layer.9.attention.output.dense.bias', 'net.vit.encoder.layer.9.intermediate.dense.weight', 'net.vit.encoder.layer.9.intermediate.dense.bias', 'net.vit.encoder.layer.9.output.dense.weight', 'net.vit.encoder.layer.9.output.dense.bias', 'net.vit.encoder.layer.9.layernorm_before.weight', 'net.vit.encoder.layer.9.layernorm_before.bias', 'net.vit.encoder.layer.9.layernorm_after.weight', 'net.vit.encoder.layer.9.layernorm_after.bias', 'net.vit.encoder.layer.10.attention.attention.query.weight', 'net.vit.encoder.layer.10.attention.attention.query.bias', 'net.vit.encoder.layer.10.attention.attention.key.weight', 'net.vit.encoder.layer.10.attention.attention.key.bias', 'net.vit.encoder.layer.10.attention.attention.value.weight', 'net.vit.encoder.layer.10.attention.attention.value.bias', 'net.vit.encoder.layer.10.attention.output.dense.weight', 'net.vit.encoder.layer.10.attention.output.dense.bias', 'net.vit.encoder.layer.10.intermediate.dense.weight', 'net.vit.encoder.layer.10.intermediate.dense.bias', 'net.vit.encoder.layer.10.output.dense.weight', 'net.vit.encoder.layer.10.output.dense.bias', 'net.vit.encoder.layer.10.layernorm_before.weight', 'net.vit.encoder.layer.10.layernorm_before.bias', 'net.vit.encoder.layer.10.layernorm_after.weight', 'net.vit.encoder.layer.10.layernorm_after.bias', 'net.vit.encoder.layer.11.attention.attention.query.weight', 'net.vit.encoder.layer.11.attention.attention.query.bias', 'net.vit.encoder.layer.11.attention.attention.key.weight', 'net.vit.encoder.layer.11.attention.attention.key.bias', 'net.vit.encoder.layer.11.attention.attention.value.weight', 'net.vit.encoder.layer.11.attention.attention.value.bias', 'net.vit.encoder.layer.11.attention.output.dense.weight', 'net.vit.encoder.layer.11.attention.output.dense.bias', 'net.vit.encoder.layer.11.intermediate.dense.weight', 'net.vit.encoder.layer.11.intermediate.dense.bias', 'net.vit.encoder.layer.11.output.dense.weight', 'net.vit.encoder.layer.11.output.dense.bias', 'net.vit.encoder.layer.11.layernorm_before.weight', 'net.vit.encoder.layer.11.layernorm_before.bias', 'net.vit.encoder.layer.11.layernorm_after.weight', 'net.vit.encoder.layer.11.layernorm_after.bias', 'net.vit.encoder.layer.12.attention.attention.query.weight', 'net.vit.encoder.layer.12.attention.attention.query.bias', 'net.vit.encoder.layer.12.attention.attention.key.weight', 'net.vit.encoder.layer.12.attention.attention.key.bias', 'net.vit.encoder.layer.12.attention.attention.value.weight', 'net.vit.encoder.layer.12.attention.attention.value.bias', 'net.vit.encoder.layer.12.attention.output.dense.weight', 'net.vit.encoder.layer.12.attention.output.dense.bias', 'net.vit.encoder.layer.12.intermediate.dense.weight', 'net.vit.encoder.layer.12.intermediate.dense.bias', 'net.vit.encoder.layer.12.output.dense.weight', 'net.vit.encoder.layer.12.output.dense.bias', 'net.vit.encoder.layer.12.layernorm_before.weight', 'net.vit.encoder.layer.12.layernorm_before.bias', 'net.vit.encoder.layer.12.layernorm_after.weight', 'net.vit.encoder.layer.12.layernorm_after.bias', 'net.vit.encoder.layer.13.attention.attention.query.weight', 'net.vit.encoder.layer.13.attention.attention.query.bias', 'net.vit.encoder.layer.13.attention.attention.key.weight', 'net.vit.encoder.layer.13.attention.attention.key.bias', 'net.vit.encoder.layer.13.attention.attention.value.weight', 'net.vit.encoder.layer.13.attention.attention.value.bias', 'net.vit.encoder.layer.13.attention.output.dense.weight', 'net.vit.encoder.layer.13.attention.output.dense.bias', 'net.vit.encoder.layer.13.intermediate.dense.weight', 'net.vit.encoder.layer.13.intermediate.dense.bias', 'net.vit.encoder.layer.13.output.dense.weight', 'net.vit.encoder.layer.13.output.dense.bias', 'net.vit.encoder.layer.13.layernorm_before.weight', 'net.vit.encoder.layer.13.layernorm_before.bias', 'net.vit.encoder.layer.13.layernorm_after.weight', 'net.vit.encoder.layer.13.layernorm_after.bias', 'net.vit.encoder.layer.14.attention.attention.query.weight', 'net.vit.encoder.layer.14.attention.attention.query.bias', 'net.vit.encoder.layer.14.attention.attention.key.weight', 'net.vit.encoder.layer.14.attention.attention.key.bias', 'net.vit.encoder.layer.14.attention.attention.value.weight', 'net.vit.encoder.layer.14.attention.attention.value.bias', 'net.vit.encoder.layer.14.attention.output.dense.weight', 'net.vit.encoder.layer.14.attention.output.dense.bias', 'net.vit.encoder.layer.14.intermediate.dense.weight', 'net.vit.encoder.layer.14.intermediate.dense.bias', 'net.vit.encoder.layer.14.output.dense.weight', 'net.vit.encoder.layer.14.output.dense.bias', 'net.vit.encoder.layer.14.layernorm_before.weight', 'net.vit.encoder.layer.14.layernorm_before.bias', 'net.vit.encoder.layer.14.layernorm_after.weight', 'net.vit.encoder.layer.14.layernorm_after.bias', 'net.vit.encoder.layer.15.attention.attention.query.weight', 'net.vit.encoder.layer.15.attention.attention.query.bias', 'net.vit.encoder.layer.15.attention.attention.key.weight', 'net.vit.encoder.layer.15.attention.attention.key.bias', 'net.vit.encoder.layer.15.attention.attention.value.weight', 'net.vit.encoder.layer.15.attention.attention.value.bias', 'net.vit.encoder.layer.15.attention.output.dense.weight', 'net.vit.encoder.layer.15.attention.output.dense.bias', 'net.vit.encoder.layer.15.intermediate.dense.weight', 'net.vit.encoder.layer.15.intermediate.dense.bias', 'net.vit.encoder.layer.15.output.dense.weight', 'net.vit.encoder.layer.15.output.dense.bias', 'net.vit.encoder.layer.15.layernorm_before.weight', 'net.vit.encoder.layer.15.layernorm_before.bias', 'net.vit.encoder.layer.15.layernorm_after.weight', 'net.vit.encoder.layer.15.layernorm_after.bias', 'net.vit.encoder.layer.16.attention.attention.query.weight', 'net.vit.encoder.layer.16.attention.attention.query.bias', 'net.vit.encoder.layer.16.attention.attention.key.weight', 'net.vit.encoder.layer.16.attention.attention.key.bias', 'net.vit.encoder.layer.16.attention.attention.value.weight', 'net.vit.encoder.layer.16.attention.attention.value.bias', 'net.vit.encoder.layer.16.attention.output.dense.weight', 'net.vit.encoder.layer.16.attention.output.dense.bias', 'net.vit.encoder.layer.16.intermediate.dense.weight', 'net.vit.encoder.layer.16.intermediate.dense.bias', 'net.vit.encoder.layer.16.output.dense.weight', 'net.vit.encoder.layer.16.output.dense.bias', 'net.vit.encoder.layer.16.layernorm_before.weight', 'net.vit.encoder.layer.16.layernorm_before.bias', 'net.vit.encoder.layer.16.layernorm_after.weight', 'net.vit.encoder.layer.16.layernorm_after.bias', 'net.vit.encoder.layer.17.attention.attention.query.weight', 'net.vit.encoder.layer.17.attention.attention.query.bias', 'net.vit.encoder.layer.17.attention.attention.key.weight', 'net.vit.encoder.layer.17.attention.attention.key.bias', 'net.vit.encoder.layer.17.attention.attention.value.weight', 'net.vit.encoder.layer.17.attention.attention.value.bias', 'net.vit.encoder.layer.17.attention.output.dense.weight', 'net.vit.encoder.layer.17.attention.output.dense.bias', 'net.vit.encoder.layer.17.intermediate.dense.weight', 'net.vit.encoder.layer.17.intermediate.dense.bias', 'net.vit.encoder.layer.17.output.dense.weight', 'net.vit.encoder.layer.17.output.dense.bias', 'net.vit.encoder.layer.17.layernorm_before.weight', 'net.vit.encoder.layer.17.layernorm_before.bias', 'net.vit.encoder.layer.17.layernorm_after.weight', 'net.vit.encoder.layer.17.layernorm_after.bias', 'net.vit.encoder.layer.18.attention.attention.query.weight', 'net.vit.encoder.layer.18.attention.attention.query.bias', 'net.vit.encoder.layer.18.attention.attention.key.weight', 'net.vit.encoder.layer.18.attention.attention.key.bias', 'net.vit.encoder.layer.18.attention.attention.value.weight', 'net.vit.encoder.layer.18.attention.attention.value.bias', 'net.vit.encoder.layer.18.attention.output.dense.weight', 'net.vit.encoder.layer.18.attention.output.dense.bias', 'net.vit.encoder.layer.18.intermediate.dense.weight', 'net.vit.encoder.layer.18.intermediate.dense.bias', 'net.vit.encoder.layer.18.output.dense.weight', 'net.vit.encoder.layer.18.output.dense.bias', 'net.vit.encoder.layer.18.layernorm_before.weight', 'net.vit.encoder.layer.18.layernorm_before.bias', 'net.vit.encoder.layer.18.layernorm_after.weight', 'net.vit.encoder.layer.18.layernorm_after.bias', 'net.vit.encoder.layer.19.attention.attention.query.weight', 'net.vit.encoder.layer.19.attention.attention.query.bias', 'net.vit.encoder.layer.19.attention.attention.key.weight', 'net.vit.encoder.layer.19.attention.attention.key.bias', 'net.vit.encoder.layer.19.attention.attention.value.weight', 'net.vit.encoder.layer.19.attention.attention.value.bias', 'net.vit.encoder.layer.19.attention.output.dense.weight', 'net.vit.encoder.layer.19.attention.output.dense.bias', 'net.vit.encoder.layer.19.intermediate.dense.weight', 'net.vit.encoder.layer.19.intermediate.dense.bias', 'net.vit.encoder.layer.19.output.dense.weight', 'net.vit.encoder.layer.19.output.dense.bias', 'net.vit.encoder.layer.19.layernorm_before.weight', 'net.vit.encoder.layer.19.layernorm_before.bias', 'net.vit.encoder.layer.19.layernorm_after.weight', 'net.vit.encoder.layer.19.layernorm_after.bias', 'net.vit.encoder.layer.20.attention.attention.query.weight', 'net.vit.encoder.layer.20.attention.attention.query.bias', 'net.vit.encoder.layer.20.attention.attention.key.weight', 'net.vit.encoder.layer.20.attention.attention.key.bias', 'net.vit.encoder.layer.20.attention.attention.value.weight', 'net.vit.encoder.layer.20.attention.attention.value.bias', 'net.vit.encoder.layer.20.attention.output.dense.weight', 'net.vit.encoder.layer.20.attention.output.dense.bias', 'net.vit.encoder.layer.20.intermediate.dense.weight', 'net.vit.encoder.layer.20.intermediate.dense.bias', 'net.vit.encoder.layer.20.output.dense.weight', 'net.vit.encoder.layer.20.output.dense.bias', 'net.vit.encoder.layer.20.layernorm_before.weight', 'net.vit.encoder.layer.20.layernorm_before.bias', 'net.vit.encoder.layer.20.layernorm_after.weight', 'net.vit.encoder.layer.20.layernorm_after.bias', 'net.vit.encoder.layer.21.attention.attention.query.weight', 'net.vit.encoder.layer.21.attention.attention.query.bias', 'net.vit.encoder.layer.21.attention.attention.key.weight', 'net.vit.encoder.layer.21.attention.attention.key.bias', 'net.vit.encoder.layer.21.attention.attention.value.weight', 'net.vit.encoder.layer.21.attention.attention.value.bias', 'net.vit.encoder.layer.21.attention.output.dense.weight', 'net.vit.encoder.layer.21.attention.output.dense.bias', 'net.vit.encoder.layer.21.intermediate.dense.weight', 'net.vit.encoder.layer.21.intermediate.dense.bias', 'net.vit.encoder.layer.21.output.dense.weight', 'net.vit.encoder.layer.21.output.dense.bias', 'net.vit.encoder.layer.21.layernorm_before.weight', 'net.vit.encoder.layer.21.layernorm_before.bias', 'net.vit.encoder.layer.21.layernorm_after.weight', 'net.vit.encoder.layer.21.layernorm_after.bias', 'net.vit.encoder.layer.22.attention.attention.query.weight', 'net.vit.encoder.layer.22.attention.attention.query.bias', 'net.vit.encoder.layer.22.attention.attention.key.weight', 'net.vit.encoder.layer.22.attention.attention.key.bias', 'net.vit.encoder.layer.22.attention.attention.value.weight', 'net.vit.encoder.layer.22.attention.attention.value.bias', 'net.vit.encoder.layer.22.attention.output.dense.weight', 'net.vit.encoder.layer.22.attention.output.dense.bias', 'net.vit.encoder.layer.22.intermediate.dense.weight', 'net.vit.encoder.layer.22.intermediate.dense.bias', 'net.vit.encoder.layer.22.output.dense.weight', 'net.vit.encoder.layer.22.output.dense.bias', 'net.vit.encoder.layer.22.layernorm_before.weight', 'net.vit.encoder.layer.22.layernorm_before.bias', 'net.vit.encoder.layer.22.layernorm_after.weight', 'net.vit.encoder.layer.22.layernorm_after.bias', 'net.vit.encoder.layer.23.attention.attention.query.weight', 'net.vit.encoder.layer.23.attention.attention.query.bias', 'net.vit.encoder.layer.23.attention.attention.key.weight', 'net.vit.encoder.layer.23.attention.attention.key.bias', 'net.vit.encoder.layer.23.attention.attention.value.weight', 'net.vit.encoder.layer.23.attention.attention.value.bias', 'net.vit.encoder.layer.23.attention.output.dense.weight', 'net.vit.encoder.layer.23.attention.output.dense.bias', 'net.vit.encoder.layer.23.intermediate.dense.weight', 'net.vit.encoder.layer.23.intermediate.dense.bias', 'net.vit.encoder.layer.23.output.dense.weight', 'net.vit.encoder.layer.23.output.dense.bias', 'net.vit.encoder.layer.23.layernorm_before.weight', 'net.vit.encoder.layer.23.layernorm_before.bias', 'net.vit.encoder.layer.23.layernorm_after.weight', 'net.vit.encoder.layer.23.layernorm_after.bias', 'net.vit.layernorm.weight', 'net.vit.layernorm.bias', 'net.decoder.mask_token', 'net.decoder.decoder_pos_embed', 'net.decoder.decoder_embed.weight', 'net.decoder.decoder_embed.bias', 'net.decoder.decoder_layers.0.attention.attention.query.weight', 'net.decoder.decoder_layers.0.attention.attention.query.bias', 'net.decoder.decoder_layers.0.attention.attention.key.weight', 'net.decoder.decoder_layers.0.attention.attention.key.bias', 'net.decoder.decoder_layers.0.attention.attention.value.weight', 'net.decoder.decoder_layers.0.attention.attention.value.bias', 'net.decoder.decoder_layers.0.attention.output.dense.weight', 'net.decoder.decoder_layers.0.attention.output.dense.bias', 'net.decoder.decoder_layers.0.intermediate.dense.weight', 'net.decoder.decoder_layers.0.intermediate.dense.bias', 'net.decoder.decoder_layers.0.output.dense.weight', 'net.decoder.decoder_layers.0.output.dense.bias', 'net.decoder.decoder_layers.0.layernorm_before.weight', 'net.decoder.decoder_layers.0.layernorm_before.bias', 'net.decoder.decoder_layers.0.layernorm_after.weight', 'net.decoder.decoder_layers.0.layernorm_after.bias', 'net.decoder.decoder_layers.1.attention.attention.query.weight', 'net.decoder.decoder_layers.1.attention.attention.query.bias', 'net.decoder.decoder_layers.1.attention.attention.key.weight', 'net.decoder.decoder_layers.1.attention.attention.key.bias', 'net.decoder.decoder_layers.1.attention.attention.value.weight', 'net.decoder.decoder_layers.1.attention.attention.value.bias', 'net.decoder.decoder_layers.1.attention.output.dense.weight', 'net.decoder.decoder_layers.1.attention.output.dense.bias', 'net.decoder.decoder_layers.1.intermediate.dense.weight', 'net.decoder.decoder_layers.1.intermediate.dense.bias', 'net.decoder.decoder_layers.1.output.dense.weight', 'net.decoder.decoder_layers.1.output.dense.bias', 'net.decoder.decoder_layers.1.layernorm_before.weight', 'net.decoder.decoder_layers.1.layernorm_before.bias', 'net.decoder.decoder_layers.1.layernorm_after.weight', 'net.decoder.decoder_layers.1.layernorm_after.bias', 'net.decoder.decoder_layers.2.attention.attention.query.weight', 'net.decoder.decoder_layers.2.attention.attention.query.bias', 'net.decoder.decoder_layers.2.attention.attention.key.weight', 'net.decoder.decoder_layers.2.attention.attention.key.bias', 'net.decoder.decoder_layers.2.attention.attention.value.weight', 'net.decoder.decoder_layers.2.attention.attention.value.bias', 'net.decoder.decoder_layers.2.attention.output.dense.weight', 'net.decoder.decoder_layers.2.attention.output.dense.bias', 'net.decoder.decoder_layers.2.intermediate.dense.weight', 'net.decoder.decoder_layers.2.intermediate.dense.bias', 'net.decoder.decoder_layers.2.output.dense.weight', 'net.decoder.decoder_layers.2.output.dense.bias', 'net.decoder.decoder_layers.2.layernorm_before.weight', 'net.decoder.decoder_layers.2.layernorm_before.bias', 'net.decoder.decoder_layers.2.layernorm_after.weight', 'net.decoder.decoder_layers.2.layernorm_after.bias', 'net.decoder.decoder_layers.3.attention.attention.query.weight', 'net.decoder.decoder_layers.3.attention.attention.query.bias', 'net.decoder.decoder_layers.3.attention.attention.key.weight', 'net.decoder.decoder_layers.3.attention.attention.key.bias', 'net.decoder.decoder_layers.3.attention.attention.value.weight', 'net.decoder.decoder_layers.3.attention.attention.value.bias', 'net.decoder.decoder_layers.3.attention.output.dense.weight', 'net.decoder.decoder_layers.3.attention.output.dense.bias', 'net.decoder.decoder_layers.3.intermediate.dense.weight', 'net.decoder.decoder_layers.3.intermediate.dense.bias', 'net.decoder.decoder_layers.3.output.dense.weight', 'net.decoder.decoder_layers.3.output.dense.bias', 'net.decoder.decoder_layers.3.layernorm_before.weight', 'net.decoder.decoder_layers.3.layernorm_before.bias', 'net.decoder.decoder_layers.3.layernorm_after.weight', 'net.decoder.decoder_layers.3.layernorm_after.bias', 'net.decoder.decoder_layers.4.attention.attention.query.weight', 'net.decoder.decoder_layers.4.attention.attention.query.bias', 'net.decoder.decoder_layers.4.attention.attention.key.weight', 'net.decoder.decoder_layers.4.attention.attention.key.bias', 'net.decoder.decoder_layers.4.attention.attention.value.weight', 'net.decoder.decoder_layers.4.attention.attention.value.bias', 'net.decoder.decoder_layers.4.attention.output.dense.weight', 'net.decoder.decoder_layers.4.attention.output.dense.bias', 'net.decoder.decoder_layers.4.intermediate.dense.weight', 'net.decoder.decoder_layers.4.intermediate.dense.bias', 'net.decoder.decoder_layers.4.output.dense.weight', 'net.decoder.decoder_layers.4.output.dense.bias', 'net.decoder.decoder_layers.4.layernorm_before.weight', 'net.decoder.decoder_layers.4.layernorm_before.bias', 'net.decoder.decoder_layers.4.layernorm_after.weight', 'net.decoder.decoder_layers.4.layernorm_after.bias', 'net.decoder.decoder_layers.5.attention.attention.query.weight', 'net.decoder.decoder_layers.5.attention.attention.query.bias', 'net.decoder.decoder_layers.5.attention.attention.key.weight', 'net.decoder.decoder_layers.5.attention.attention.key.bias', 'net.decoder.decoder_layers.5.attention.attention.value.weight', 'net.decoder.decoder_layers.5.attention.attention.value.bias', 'net.decoder.decoder_layers.5.attention.output.dense.weight', 'net.decoder.decoder_layers.5.attention.output.dense.bias', 'net.decoder.decoder_layers.5.intermediate.dense.weight', 'net.decoder.decoder_layers.5.intermediate.dense.bias', 'net.decoder.decoder_layers.5.output.dense.weight', 'net.decoder.decoder_layers.5.output.dense.bias', 'net.decoder.decoder_layers.5.layernorm_before.weight', 'net.decoder.decoder_layers.5.layernorm_before.bias', 'net.decoder.decoder_layers.5.layernorm_after.weight', 'net.decoder.decoder_layers.5.layernorm_after.bias', 'net.decoder.decoder_layers.6.attention.attention.query.weight', 'net.decoder.decoder_layers.6.attention.attention.query.bias', 'net.decoder.decoder_layers.6.attention.attention.key.weight', 'net.decoder.decoder_layers.6.attention.attention.key.bias', 'net.decoder.decoder_layers.6.attention.attention.value.weight', 'net.decoder.decoder_layers.6.attention.attention.value.bias', 'net.decoder.decoder_layers.6.attention.output.dense.weight', 'net.decoder.decoder_layers.6.attention.output.dense.bias', 'net.decoder.decoder_layers.6.intermediate.dense.weight', 'net.decoder.decoder_layers.6.intermediate.dense.bias', 'net.decoder.decoder_layers.6.output.dense.weight', 'net.decoder.decoder_layers.6.output.dense.bias', 'net.decoder.decoder_layers.6.layernorm_before.weight', 'net.decoder.decoder_layers.6.layernorm_before.bias', 'net.decoder.decoder_layers.6.layernorm_after.weight', 'net.decoder.decoder_layers.6.layernorm_after.bias', 'net.decoder.decoder_layers.7.attention.attention.query.weight', 'net.decoder.decoder_layers.7.attention.attention.query.bias', 'net.decoder.decoder_layers.7.attention.attention.key.weight', 'net.decoder.decoder_layers.7.attention.attention.key.bias', 'net.decoder.decoder_layers.7.attention.attention.value.weight', 'net.decoder.decoder_layers.7.attention.attention.value.bias', 'net.decoder.decoder_layers.7.attention.output.dense.weight', 'net.decoder.decoder_layers.7.attention.output.dense.bias', 'net.decoder.decoder_layers.7.intermediate.dense.weight', 'net.decoder.decoder_layers.7.intermediate.dense.bias', 'net.decoder.decoder_layers.7.output.dense.weight', 'net.decoder.decoder_layers.7.output.dense.bias', 'net.decoder.decoder_layers.7.layernorm_before.weight', 'net.decoder.decoder_layers.7.layernorm_before.bias', 'net.decoder.decoder_layers.7.layernorm_after.weight', 'net.decoder.decoder_layers.7.layernorm_after.bias', 'net.decoder.decoder_norm.weight', 'net.decoder.decoder_norm.bias', 'net.decoder.decoder_pred.weight', 'net.decoder.decoder_pred.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc887bf6-fe26-448f-b742-782f5d8aaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = VisionTransformerMAE(image_size = 3072, patch_size = 48)\n",
    "mae.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a2b4bd-ba0e-4d96-890b-8d7e695159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerMAE(\n",
       "  (net): ViTMAEForPreTraining(\n",
       "    (vit): ViTMAEModel(\n",
       "      (embeddings): ViTMAEEmbeddings(\n",
       "        (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "          (projection): Conv2d(1, 768, kernel_size=(48, 48), stride=(48, 48))\n",
       "        )\n",
       "      )\n",
       "      (encoder): ViTMAEEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x ViTMAELayer(\n",
       "            (attention): ViTMAEAttention(\n",
       "              (attention): ViTMAESelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTMAESelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTMAEIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTMAEOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): ViTMAEDecoder(\n",
       "      (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (decoder_layers): ModuleList(\n",
       "        (0-7): 8 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (decoder_pred): Linear(in_features=512, out_features=2304, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_loss): MeanSquaredError()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3757896c-6ad3-4c1d-9f2c-f724bba1f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable masking\n",
    "#mae.net.config.mask_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1352bd16-65db-44fd-b7bd-0b66fc4c7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_attentions: True\n",
      "_attn_implementation: eager\n"
     ]
    }
   ],
   "source": [
    "# For outputting attentions\n",
    "print('output_attentions:', mae.net.config.output_attentions)\n",
    "print('_attn_implementation:', mae.net.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733f1ff2-9a6a-40ff-a48d-29239a4de188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing MRIDatasetBase ...\n",
      "reading /home/buehlern/Documents/Masterarbeit/data/clean_df_slim_frac.pkl file ...\n",
      "PATH /home/buehlern/Documents/Masterarbeit/data/BodyPartExamined_mappings_mergemore.json\n",
      "/home/buehlern/Documents/Masterarbeit/data/cache-full/df_labelcomparison.pkl does not exit --> no items excluded by it\n",
      "MRIDatasetBase(len=639877) initialized\n",
      "\n",
      "initializing MRIDataset(mode=train) ...\n",
      "MRIDataset(mode=train, len=516402) initialized\n",
      "\n",
      "initializing MRIDataset(mode=val) ...\n",
      "MRIDataset(mode=val, len=27518) initialized\n",
      "\n",
      "initializing MRIDataset(mode=test) ...\n",
      "WARN: including test data\n",
      "MRIDataset(mode=test, len=95957) initialized\n"
     ]
    }
   ],
   "source": [
    "# Load the DataModule\n",
    "mri_datamodule = MRIDataModule(image_size = 3072, square = True, output_channels = 1, cache = False, fix_inverted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "617949f2-82d8-4ea2-8dd9-20c5f180f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 1]\n",
    "    assert image.shape[2] == 1\n",
    "    plt.imshow(image, cmap=plt.cm.bone)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d002e0b4-3ea3-4e81-8260-0d16e80c7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = iter(mri_datamodule.data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56503f00-3c36-45f3-9d5b-ffda7674b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pixel_values, model, imgname=None):\n",
    "    image_size = model.config.image_size\n",
    "    patch_size = model.config.patch_size\n",
    "    num_patches = image_size // patch_size\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(pixel_values, output_attentions=True)\n",
    "    y = model.unpatchify(outputs.logits)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "    \n",
    "    # visualize the mask\n",
    "    mask = outputs.mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 *1)  # (N, H*W, p*p*1)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    \n",
    "    x = torch.einsum('nchw->nhwc', pixel_values)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # Attention map calculations\n",
    "    # seq_len = 1025\n",
    "    # Without [CLS] token: seq_len = 1024\n",
    "    # num_patches_x_out = num_patches_y = sqrt(seq_len) = sqrt(1024) = 32\n",
    "    # num_patches_x_in = image_size // patch_size = 3072 // 48 = 64\n",
    "    # Reason for difference is masking! 4096 * (1-0.75) = 1024\n",
    "\n",
    "    #image_size = model.config.image_size\n",
    "    #patch_size = model.config.patch_size\n",
    "    #num_patches = image_size // patch_size\n",
    "    #attn_map = attn.squeeze().numpy()\n",
    "    ## Scale attention map to original image size\n",
    "    #attn_map_scaled = np.zeros((image_size, image_size))\n",
    "    #for i in range(num_patches):\n",
    "    #    for j in range(num_patches):\n",
    "    #        x_start = i * patch_size\n",
    "    #        x_end = x_start + patch_size\n",
    "    #        y_start = j * patch_size\n",
    "    #        y_end = y_start + patch_size\n",
    "    #        \n",
    "    #        attn_map_scaled[x_start:x_end, y_start:y_end] = attn_map[i * num_patches + j]\n",
    "    \n",
    "    #num_patches = attn.size(-1)\n",
    "    #print(f\"num_patches (after masking {model.config.mask_ratio}):\", num_patches) # 1024\n",
    "    #attn = attn[0].reshape(model.config.patch_size, model.config.patch_size, -1)\n",
    "    # Rescale attention to image size\n",
    "    #attn = F.interpolate(attn.unsqueeze(0).unsqueeze(0), scale_factor=model.config.patch_size, mode=\"nearest\")[0][0]\n",
    "\n",
    "    attentions = outputs.attentions\n",
    "    print(\"len(attentions):\", len(attentions)) # 24 layers\n",
    "    print(\"attentions[0].shape:\", attentions[0].shape) # [1, 16, 1025, 1025]\n",
    "    # Initialize full attention map\n",
    "    full_attn_map = torch.zeros((1, image_size, image_size)) # (1, 3072, 3072)\n",
    "    masklist = outputs.mask.detach().type(torch.int64) > 0\n",
    "    print(\"masklist.shape:\", masklist.shape) # (1, 4096)\n",
    "    # Determine patch contributions\n",
    "    #patch_contrib = attentions[-1][:, :, 1:, 1:].mean(dim=1) # Take mean attention of last layer (without [CLS] token)\n",
    "    patch_contrib = torch.zeros(attentions[0].shape[-1]-1) # -1 for [CLS] token\n",
    "    check_cls_token = False\n",
    "    if check_cls_token:\n",
    "        for layer_attn in attentions:\n",
    "            # Only check [CLS] token\n",
    "            attn = layer_attn[:, :, 1, 1:].detach().cpu() # [1, 16, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1, 1024]\n",
    "            patch_contrib += attn\n",
    "    else:\n",
    "        for layer_attn in attentions:\n",
    "            # Remove [CLS] token\n",
    "            attn = layer_attn[:, :, 1:, 1:].detach().cpu() # [1, 16, 1024, 1024]\n",
    "            # Average over heads\n",
    "            attn = attn.mean(dim=1) # [1, 1024, 1024]\n",
    "            # Average over batch (if batch_size > 1)\n",
    "            attn = attn.mean(dim=0) # [1024, 1024]\n",
    "            # Average contribution across all other tokens\n",
    "            attn = attn.mean(dim=1) # [1024,]\n",
    "            patch_contrib += attn\n",
    "    # Normalize\n",
    "    patch_contrib -= patch_contrib.min()\n",
    "    patch_contrib /= patch_contrib.max()\n",
    "    print(\"patch_contrib.shape:\", patch_contrib.shape)\n",
    "    print(pd.DataFrame(patch_contrib).describe())\n",
    "    # Map attention scores onto full attention map\n",
    "    attn_iter = iter(patch_contrib)\n",
    "    print(\"len(masklist[0]):\", len(masklist[0]))\n",
    "    for i, masked in enumerate(masklist[0]):\n",
    "        row = i // num_patches\n",
    "        col = i % num_patches\n",
    "        #print(\"i, row, col, masked:\", i, row, col, masked.item())\n",
    "        attn_val = next(attn_iter) if ~masked else torch.tensor([0])\n",
    "        full_attn_map[0, row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size] = attn_val.expand((patch_size, patch_size))\n",
    "    \n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 10]\n",
    "\n",
    "    plt.subplot(1, 5, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 5, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 5, 3)\n",
    "    show_image(y[0], f\"reconstruction (loss: {outputs.loss.item():.4f})\")\n",
    "\n",
    "    plt.subplot(1, 5, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "    \n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(full_attn_map[0], cmap='gray', interpolation='nearest')\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    if imgname is not None:\n",
    "        plt.savefig('/home/buehlern/Documents/Masterarbeit/notebooks/Data Exploration Graphics/Model Eval/ViT MAE/' + str(imgname) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4e25-0ada-4aea-b418-c334694b04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(dl_iter)\n",
    "image = item[0]\n",
    "batch = image.unsqueeze(0)\n",
    "visualize(batch, mae.net, imgname=\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58102e24-6696-472a-8853-e03822a5e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuf_dl_iter = iter(random.sample(list(mri_datamodule.data_val), len(mri_datamodule.data_val)))\n",
    "#for i in range(50):\n",
    "#    item = next(shuf_dl_iter)\n",
    "#    image = item[0]\n",
    "#    batch = image.unsqueeze(0)\n",
    "#    visualize(batch, mae.net, imgname=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

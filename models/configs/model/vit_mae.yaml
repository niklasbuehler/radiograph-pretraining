_target_: src.models.vit_mae_module.VisionTransformerMAE

image_size: 3072
patch_size: 48
image_channels: 1
# compile model for faster training with pytorch 2.0
compile: true

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1.5e-4
  weight_decay: 0.05
  betas:
  - 0.9
  - 0.95

#scheduler:
#  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#  _partial_: true
#  T_max: 5

#scheduler:
#  _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
#  _partial_: true
#  warmup_start_lr: 1e-5
#  warmup_epochs: 1
#  max_epochs: 3

#scheduler:
#  _target_: src.optim.WarmupCosineAnnealingLR
#  _partial_: true
#  eta_min: 1e-5
#  warmup_steps: 10 # 32276 # batches per epoch
#  max_steps: 100 # 96828 # = 32276*3 batches in total

#scheduler:
#  warmup_epochs: 10 #32276
#  max_epochs: 3000 #96828 # 32276*3
#  warmup_start_lr: 1e-5

scheduler:
  _target_: src.optim.warmup_cosine_annealing_lr_scheduler.WarmupCosineAnnealingLR
  _partial_: true
  eta_min: 1e-5
  warmup_steps: 504 # 32276/64*1 = num_train_batches / grad_acc * warmup_epochs
  max_steps: 1513 # = 32276/64*3 = num_train_batches / grad_acc * total_epochs
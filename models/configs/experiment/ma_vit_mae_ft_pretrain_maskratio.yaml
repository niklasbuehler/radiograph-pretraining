# @package _global_

defaults:
  - ma_vit_mae_ft_pretrain

trainer:
  accumulate_grad_batches: 128 # keep the same effective batch_size
  
model:
  mask_ratio: 0.5 # 0.75 -> 0.5 doubles memory consumption

data:
  batch_size: 8 # half batch size (due to memory constraints)
  num_workers: 4
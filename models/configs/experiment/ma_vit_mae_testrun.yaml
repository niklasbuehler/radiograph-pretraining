# @package _global_

defaults:
  - override /data: mri
  - override /model: vit_mae
  - override /callbacks: mae_no_checkpoints
  - override /trainer: gpu
  - override /logger: wandb

tags: ["ma", "mri", "vit", "mae", "testrun"]

seed: 12345

trainer:
  min_epochs: 1 # prevents early stopping
  max_epochs: 3
  accumulate_grad_batches: 1 # 64 # effective batch size = 64*batch_size
  log_every_n_steps: 1

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1.5e-4
    weight_decay: 0.05
    betas:
    - 0.9
    - 0.95
  compile: false

data:
  #df_name: df_min_ft_test_114
  total_data_size: 1000
  batch_size: 16
  num_workers: 64
  persistent_workers: True
  batch_binning: smart
  batch_bins: [1152, 1536, 1920, 2304, 2688, 3072]
  label: fracture

logger:
  wandb:
    tags: ${tags}

# @package _global_

defaults:
  - override /data: mri
  - override /model: swin_mae
  - override /callbacks: mae_no_checkpoints
  - override /trainer: gpu
  - override /logger: wandb

tags: ["ma", "mri", "swin", "mae", "testrun"]

seed: 12345

trainer:
  min_epochs: 1 # prevents early stopping
  max_epochs: 10
  accumulate_grad_batches: 1

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.001
    weight_decay: 0.05
    betas:
    - 0.9
    - 0.95
  image_size: 3072
  patch_size: 8 # 16 # 4
  encoder_stride: 64 # 128 # 32
  # window_size: 24
  scheduler:
    _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
    _partial_: true
    warmup_start_lr: 1e-5
    warmup_epochs: 5
    max_epochs: 10

data:
  total_data_size: 500
  batch_size: 1
  num_workers: 2
  persistent_workers: True
  #image_size: 3072
  square: False
  pad_to_multiple_of: 8 # = patch_size
  output_channels: 1

logger:
  wandb:
    tags: ${tags}

# @package _global_

defaults:
  - override /data: mri
  - override /model: vit_mae
  - override /callbacks: mae
  - override /trainer: gpu
  - override /logger: wandb

tags: ["ma", "mri", "vit", "mae"]

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 10
  accumulate_grad_batches: 64

model:
  compile: false
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0.05
    betas:
    - 0.9
    - 0.95
  scheduler:
    # These parameters should be invariant under same effective batch_size
    # (batch_size & accumulate_grad_batches)
    warmup_steps: 2520 # 32276/64*5 = num_train_batches / grad_acc * warmup_epochs
    max_steps: 5040 # = 32276/64*10 = num_train_batches / grad_acc * total_epochs

data:
  train_augmentations: false
  batch_size: 16 # Make sure to adapt accumulate_grad_batches
  num_workers: 32
  batch_binning: smart
  batch_bins: [1152, 1536, 1920, 2304, 2688, 3072]
  val_size: 0.05
  test_size: 0.15

logger:
  wandb:
    tags: ${tags}
